* 
* ==> Audit <==
* |--------------|--------------------------------|----------|---------------|---------|---------------------|---------------------|
|   Command    |              Args              | Profile  |     User      | Version |     Start Time      |      End Time       |
|--------------|--------------------------------|----------|---------------|---------|---------------------|---------------------|
| stop         |                                | minikube | MY_KIKI\omerm | v1.32.0 | 06 Mar 24 18:11 IST | 06 Mar 24 18:11 IST |
| start        |                                | minikube | MY_KIKI\omerm | v1.32.0 | 15 Apr 25 16:52 IST | 15 Apr 25 16:57 IST |
| start        |                                | minikube | MY_KIKI\omerm | v1.32.0 | 23 Apr 25 21:12 IST | 23 Apr 25 21:13 IST |
| service      | voting-service --url           | minikube | MY_KIKI\omerm | v1.32.0 | 24 Apr 25 00:41 IST |                     |
| service      | voting-service --url           | minikube | MY_KIKI\omerm | v1.32.0 | 24 Apr 25 00:42 IST |                     |
| service      | voting-service --url           | minikube | MY_KIKI\omerm | v1.32.0 | 24 Apr 25 00:45 IST |                     |
| ip           |                                | minikube | MY_KIKI\omerm | v1.32.0 | 24 Apr 25 00:45 IST | 24 Apr 25 00:45 IST |
| service      | voting-service --url           | minikube | MY_KIKI\omerm | v1.32.0 | 24 Apr 25 00:53 IST | 24 Apr 25 00:54 IST |
| ip           |                                | minikube | MY_KIKI\omerm | v1.32.0 | 24 Apr 25 00:55 IST | 24 Apr 25 00:55 IST |
| ip           |                                | minikube | MY_KIKI\omerm | v1.32.0 | 24 Apr 25 00:58 IST | 24 Apr 25 00:58 IST |
| service      | voting-service --url           | minikube | MY_KIKI\omerm | v1.32.0 | 24 Apr 25 01:03 IST | 24 Apr 25 01:04 IST |
| service      | voting-service --url           | minikube | MY_KIKI\omerm | v1.32.0 | 24 Apr 25 01:21 IST | 24 Apr 25 01:24 IST |
| service      | voting-service --url           | minikube | MY_KIKI\omerm | v1.32.0 | 24 Apr 25 01:51 IST |                     |
| service      | voting-service --url           | minikube | MY_KIKI\omerm | v1.32.0 | 24 Apr 25 01:52 IST |                     |
| service      | voting-service --url           | minikube | MY_KIKI\omerm | v1.32.0 | 24 Apr 25 01:56 IST |                     |
| service      | result-service --url           | minikube | MY_KIKI\omerm | v1.32.0 | 24 Apr 25 01:59 IST | 24 Apr 25 02:01 IST |
| stop         |                                | minikube | MY_KIKI\omerm | v1.32.0 | 24 Apr 25 02:02 IST | 24 Apr 25 02:02 IST |
| start        |                                | minikube | MY_KIKI\omerm | v1.32.0 | 06 May 25 16:16 IST | 06 May 25 16:17 IST |
| service      | web-service                    | minikube | MY_KIKI\omerm | v1.32.0 | 06 May 25 17:08 IST | 06 May 25 17:08 IST |
| service      | hello-service                  | minikube | MY_KIKI\omerm | v1.32.0 | 06 May 25 17:26 IST | 06 May 25 17:26 IST |
| service      | hello-service                  | minikube | MY_KIKI\omerm | v1.32.0 | 06 May 25 17:30 IST | 06 May 25 17:30 IST |
| start        |                                | minikube | MY_KIKI\omerm | v1.32.0 | 10 May 25 19:36 IST | 10 May 25 19:37 IST |
| stop         |                                | minikube | MY_KIKI\omerm | v1.32.0 | 11 May 25 22:49 IST | 11 May 25 22:49 IST |
| update-check |                                | minikube | MY_KIKI\omerm | v1.32.0 | 30 May 25 22:29 IST | 30 May 25 22:29 IST |
| update-check |                                | minikube | MY_KIKI\omerm | v1.32.0 | 31 May 25 20:48 IST | 31 May 25 20:48 IST |
| update-check |                                | minikube | MY_KIKI\omerm | v1.32.0 | 08 Jun 25 08:49 IST | 08 Jun 25 08:49 IST |
| update-check |                                | minikube | MY_KIKI\omerm | v1.32.0 | 08 Jun 25 17:53 IST | 08 Jun 25 17:53 IST |
| update-check |                                | minikube | MY_KIKI\omerm | v1.32.0 | 11 Jun 25 18:24 IST | 11 Jun 25 18:24 IST |
| update-check |                                | minikube | MY_KIKI\omerm | v1.32.0 | 12 Jun 25 16:58 IST | 12 Jun 25 16:58 IST |
| update-check |                                | minikube | MY_KIKI\omerm | v1.32.0 | 16 Jun 25 19:25 IST | 16 Jun 25 19:25 IST |
| update-check |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 15:15 IST | 19 Jun 25 15:15 IST |
| start        |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 18:05 IST | 19 Jun 25 18:06 IST |
| addons       | enable ingress                 | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 18:23 IST | 19 Jun 25 18:23 IST |
| ip           |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 18:44 IST | 19 Jun 25 18:44 IST |
| tunnel       |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 18:49 IST | 19 Jun 25 18:51 IST |
| service      | react-service --url            | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 19:17 IST | 19 Jun 25 19:17 IST |
| service      | react-service --url            | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 19:17 IST | 19 Jun 25 19:17 IST |
| addons       | enable ingress                 | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 19:37 IST | 19 Jun 25 19:37 IST |
| ip           |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 19:38 IST | 19 Jun 25 19:38 IST |
| tunnel       |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 19:40 IST |                     |
| ip           |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 19:48 IST | 19 Jun 25 19:48 IST |
| tunnel       |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 20:10 IST |                     |
| tunnel       |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 20:14 IST |                     |
| tunnel       |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 20:19 IST | 19 Jun 25 20:32 IST |
| tunnel       |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 20:35 IST | 19 Jun 25 20:41 IST |
| ip           |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 20:44 IST | 19 Jun 25 20:44 IST |
| ip           |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 20:52 IST | 19 Jun 25 20:52 IST |
| ip           |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 21:25 IST | 19 Jun 25 21:25 IST |
| ip           |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 21:27 IST | 19 Jun 25 21:27 IST |
| ip           |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 23:29 IST | 19 Jun 25 23:29 IST |
| ip           |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 23:34 IST | 19 Jun 25 23:34 IST |
| tunnel       |                                | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 23:38 IST | 19 Jun 25 23:39 IST |
| service      | backend-service --url          | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 23:48 IST | 19 Jun 25 23:53 IST |
| addons       | enable ingress                 | minikube | MY_KIKI\omerm | v1.32.0 | 19 Jun 25 23:53 IST | 19 Jun 25 23:53 IST |
| ip           |                                | minikube | MY_KIKI\omerm | v1.32.0 | 20 Jun 25 01:38 IST | 20 Jun 25 01:38 IST |
| service      | backend-service --url          | minikube | MY_KIKI\omerm | v1.32.0 | 20 Jun 25 01:54 IST | 20 Jun 25 01:55 IST |
| service      | backend-service                | minikube | MY_KIKI\omerm | v1.32.0 | 20 Jun 25 01:55 IST | 20 Jun 25 01:56 IST |
| update-check |                                | minikube | MY_KIKI\omerm | v1.32.0 | 20 Jun 25 14:14 IST | 20 Jun 25 14:14 IST |
| start        |                                | minikube | MY_KIKI\omerm | v1.32.0 | 20 Jun 25 18:41 IST | 20 Jun 25 18:42 IST |
| service      | -n monitoring                  | minikube | MY_KIKI\omerm | v1.32.0 | 20 Jun 25 23:33 IST |                     |
|              | monitoring-grafana --url       |          |               |         |                     |                     |
|--------------|--------------------------------|----------|---------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2025/06/20 18:41:42
Running on machine: MY_KIKI
Binary: Built with gc go1.21.3 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0620 18:41:42.596305   19268 out.go:296] Setting OutFile to fd 1128 ...
I0620 18:41:42.596305   19268 out.go:348] isatty.IsTerminal(1128) = false
I0620 18:41:42.596305   19268 out.go:309] Setting ErrFile to fd 1128...
I0620 18:41:42.596305   19268 out.go:348] isatty.IsTerminal(1128) = false
I0620 18:41:42.630251   19268 out.go:303] Setting JSON to false
I0620 18:41:42.636928   19268 start.go:128] hostinfo: {"hostname":"MY_KIKI","uptime":496503,"bootTime":1749928599,"procs":290,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.4351 Build 26100.4351","kernelVersion":"10.0.26100.4351 Build 26100.4351","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"4e3d923d-47cb-4a65-93f5-21f84cd4dcb1"}
W0620 18:41:42.636928   19268 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0620 18:41:42.639091   19268 out.go:177] * minikube v1.32.0 on Microsoft Windows 11 Home Single Language 10.0.26100.4351 Build 26100.4351
I0620 18:41:42.641940   19268 notify.go:220] Checking for updates...
I0620 18:41:42.643105   19268 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0620 18:41:42.645492   19268 driver.go:378] Setting default libvirt URI to qemu:///system
I0620 18:41:42.854680   19268 docker.go:122] docker version: linux-24.0.7:Docker Desktop 4.26.0 (130397)
I0620 18:41:42.872992   19268 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0620 18:41:44.455475   19268 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.5824833s)
I0620 18:41:44.457392   19268 info.go:266] docker info: {ID:97f1c12b-04d1-48af-952e-e2abdb51f603 Containers:11 ContainersRunning:1 ContainersPaused:0 ContainersStopped:10 Images:12 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:53 OomKillDisable:true NGoroutines:61 SystemTime:2025-06-20 13:11:44.399078288 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:8258928640 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f Expected:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f} RuncCommit:{ID:v1.1.10-0-g18a0cb0 Expected:v1.1.10-0-g18a0cb0} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.0-desktop.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.3-desktop.2] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:0.1] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.10] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.2.0]] Warnings:<nil>}}
I0620 18:41:44.459210   19268 out.go:177] * Using the docker driver based on existing profile
I0620 18:41:44.460984   19268 start.go:298] selected driver: docker
I0620 18:41:44.460984   19268 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\omerm:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0620 18:41:44.460984   19268 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0620 18:41:44.489375   19268 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0620 18:41:44.827485   19268 info.go:266] docker info: {ID:97f1c12b-04d1-48af-952e-e2abdb51f603 Containers:11 ContainersRunning:1 ContainersPaused:0 ContainersStopped:10 Images:12 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:53 OomKillDisable:true NGoroutines:61 SystemTime:2025-06-20 13:11:44.787482769 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:8258928640 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f Expected:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f} RuncCommit:{ID:v1.1.10-0-g18a0cb0 Expected:v1.1.10-0-g18a0cb0} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.0-desktop.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.3-desktop.2] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:0.1] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.10] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.2.0]] Warnings:<nil>}}
I0620 18:41:44.887346   19268 cni.go:84] Creating CNI manager for ""
I0620 18:41:44.887346   19268 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0620 18:41:44.887346   19268 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\omerm:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0620 18:41:44.889955   19268 out.go:177] * Starting control plane node minikube in cluster minikube
I0620 18:41:44.892926   19268 cache.go:121] Beginning downloading kic base image for docker with docker
I0620 18:41:44.894916   19268 out.go:177] * Pulling base image ...
I0620 18:41:44.898258   19268 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0620 18:41:44.898258   19268 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0620 18:41:44.898814   19268 preload.go:148] Found local preload: C:\Users\omerm\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0620 18:41:44.898814   19268 cache.go:56] Caching tarball of preloaded images
I0620 18:41:44.899371   19268 preload.go:174] Found C:\Users\omerm\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0620 18:41:44.899371   19268 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0620 18:41:44.900635   19268 profile.go:148] Saving config to C:\Users\omerm\.minikube\profiles\minikube\config.json ...
I0620 18:41:45.076947   19268 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0620 18:41:45.077654   19268 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0620 18:41:45.078401   19268 cache.go:194] Successfully downloaded all kic artifacts
I0620 18:41:45.079054   19268 start.go:365] acquiring machines lock for minikube: {Name:mk60e7f732297315c1b2f7f7ef1cc7f750810097 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0620 18:41:45.079718   19268 start.go:369] acquired machines lock for "minikube" in 663.9Âµs
I0620 18:41:45.079718   19268 start.go:96] Skipping create...Using existing machine configuration
I0620 18:41:45.080309   19268 fix.go:54] fixHost starting: 
I0620 18:41:45.140278   19268 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0620 18:41:45.295476   19268 fix.go:102] recreateIfNeeded on minikube: state=Running err=<nil>
W0620 18:41:45.295476   19268 fix.go:128] unexpected machine state, will restart: <nil>
I0620 18:41:45.298372   19268 out.go:177] * Updating the running docker "minikube" container ...
I0620 18:41:45.300248   19268 machine.go:88] provisioning docker machine ...
I0620 18:41:45.300816   19268 ubuntu.go:169] provisioning hostname "minikube"
I0620 18:41:45.312481   19268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 18:41:45.520173   19268 main.go:141] libmachine: Using SSH client type: native
I0620 18:41:45.521585   19268 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe147e0] 0xe17320 <nil>  [] 0s} 127.0.0.1 27276 <nil> <nil>}
I0620 18:41:45.521585   19268 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0620 18:41:45.726137   19268 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0620 18:41:45.740423   19268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 18:41:45.891988   19268 main.go:141] libmachine: Using SSH client type: native
I0620 18:41:45.892597   19268 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe147e0] 0xe17320 <nil>  [] 0s} 127.0.0.1 27276 <nil> <nil>}
I0620 18:41:45.892597   19268 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0620 18:41:46.026350   19268 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0620 18:41:46.035956   19268 ubuntu.go:175] set auth options {CertDir:C:\Users\omerm\.minikube CaCertPath:C:\Users\omerm\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\omerm\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\omerm\.minikube\machines\server.pem ServerKeyPath:C:\Users\omerm\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\omerm\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\omerm\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\omerm\.minikube}
I0620 18:41:46.035956   19268 ubuntu.go:177] setting up certificates
I0620 18:41:46.035956   19268 provision.go:83] configureAuth start
I0620 18:41:46.051149   19268 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0620 18:41:46.199543   19268 provision.go:138] copyHostCerts
I0620 18:41:46.223377   19268 exec_runner.go:144] found C:\Users\omerm\.minikube/key.pem, removing ...
I0620 18:41:46.223377   19268 exec_runner.go:203] rm: C:\Users\omerm\.minikube\key.pem
I0620 18:41:46.223377   19268 exec_runner.go:151] cp: C:\Users\omerm\.minikube\certs\key.pem --> C:\Users\omerm\.minikube/key.pem (1675 bytes)
I0620 18:41:46.240651   19268 exec_runner.go:144] found C:\Users\omerm\.minikube/ca.pem, removing ...
I0620 18:41:46.240651   19268 exec_runner.go:203] rm: C:\Users\omerm\.minikube\ca.pem
I0620 18:41:46.241269   19268 exec_runner.go:151] cp: C:\Users\omerm\.minikube\certs\ca.pem --> C:\Users\omerm\.minikube/ca.pem (1074 bytes)
I0620 18:41:46.255399   19268 exec_runner.go:144] found C:\Users\omerm\.minikube/cert.pem, removing ...
I0620 18:41:46.255399   19268 exec_runner.go:203] rm: C:\Users\omerm\.minikube\cert.pem
I0620 18:41:46.256280   19268 exec_runner.go:151] cp: C:\Users\omerm\.minikube\certs\cert.pem --> C:\Users\omerm\.minikube/cert.pem (1119 bytes)
I0620 18:41:46.256798   19268 provision.go:112] generating server cert: C:\Users\omerm\.minikube\machines\server.pem ca-key=C:\Users\omerm\.minikube\certs\ca.pem private-key=C:\Users\omerm\.minikube\certs\ca-key.pem org=omerm.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0620 18:41:46.443047   19268 provision.go:172] copyRemoteCerts
I0620 18:41:46.473029   19268 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0620 18:41:46.488723   19268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 18:41:46.682458   19268 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:27276 SSHKeyPath:C:\Users\omerm\.minikube\machines\minikube\id_rsa Username:docker}
I0620 18:41:46.814999   19268 ssh_runner.go:362] scp C:\Users\omerm\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0620 18:41:46.916314   19268 ssh_runner.go:362] scp C:\Users\omerm\.minikube\machines\server.pem --> /etc/docker/server.pem (1200 bytes)
I0620 18:41:47.002677   19268 ssh_runner.go:362] scp C:\Users\omerm\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0620 18:41:47.049201   19268 provision.go:86] duration metric: configureAuth took 1.0132448s
I0620 18:41:47.049795   19268 ubuntu.go:193] setting minikube options for container-runtime
I0620 18:41:47.050340   19268 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0620 18:41:47.063178   19268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 18:41:47.219422   19268 main.go:141] libmachine: Using SSH client type: native
I0620 18:41:47.219422   19268 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe147e0] 0xe17320 <nil>  [] 0s} 127.0.0.1 27276 <nil> <nil>}
I0620 18:41:47.219422   19268 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0620 18:41:47.366687   19268 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0620 18:41:47.366687   19268 ubuntu.go:71] root file system type: overlay
I0620 18:41:47.366687   19268 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0620 18:41:47.380341   19268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 18:41:47.530719   19268 main.go:141] libmachine: Using SSH client type: native
I0620 18:41:47.531323   19268 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe147e0] 0xe17320 <nil>  [] 0s} 127.0.0.1 27276 <nil> <nil>}
I0620 18:41:47.531323   19268 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0620 18:41:47.677218   19268 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0620 18:41:47.690336   19268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 18:41:47.842195   19268 main.go:141] libmachine: Using SSH client type: native
I0620 18:41:47.842799   19268 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe147e0] 0xe17320 <nil>  [] 0s} 127.0.0.1 27276 <nil> <nil>}
I0620 18:41:47.842799   19268 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0620 18:41:47.995850   19268 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0620 18:41:47.995850   19268 machine.go:91] provisioned docker machine in 2.6956018s
I0620 18:41:47.995850   19268 start.go:300] post-start starting for "minikube" (driver="docker")
I0620 18:41:47.995850   19268 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0620 18:41:48.016376   19268 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0620 18:41:48.028540   19268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 18:41:48.180699   19268 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:27276 SSHKeyPath:C:\Users\omerm\.minikube\machines\minikube\id_rsa Username:docker}
I0620 18:41:48.285381   19268 ssh_runner.go:195] Run: cat /etc/os-release
I0620 18:41:48.290975   19268 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0620 18:41:48.290975   19268 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0620 18:41:48.290975   19268 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0620 18:41:48.290975   19268 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0620 18:41:48.290975   19268 filesync.go:126] Scanning C:\Users\omerm\.minikube\addons for local assets ...
I0620 18:41:48.292046   19268 filesync.go:126] Scanning C:\Users\omerm\.minikube\files for local assets ...
I0620 18:41:48.292046   19268 start.go:303] post-start completed in 296.1958ms
I0620 18:41:48.295804   19268 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0620 18:41:48.311502   19268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 18:41:48.460869   19268 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:27276 SSHKeyPath:C:\Users\omerm\.minikube\machines\minikube\id_rsa Username:docker}
I0620 18:41:48.569297   19268 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0620 18:41:48.584388   19268 fix.go:56] fixHost completed within 3.5046698s
I0620 18:41:48.584388   19268 start.go:83] releasing machines lock for "minikube", held for 3.5046698s
I0620 18:41:48.618491   19268 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0620 18:41:48.823101   19268 ssh_runner.go:195] Run: cat /version.json
I0620 18:41:48.831429   19268 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0620 18:41:48.841460   19268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 18:41:48.849612   19268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 18:41:49.003016   19268 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:27276 SSHKeyPath:C:\Users\omerm\.minikube\machines\minikube\id_rsa Username:docker}
I0620 18:41:49.016923   19268 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:27276 SSHKeyPath:C:\Users\omerm\.minikube\machines\minikube\id_rsa Username:docker}
I0620 18:41:49.153407   19268 ssh_runner.go:195] Run: systemctl --version
I0620 18:41:49.429028   19268 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0620 18:41:49.471337   19268 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0620 18:41:49.484747   19268 start.go:416] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0620 18:41:49.501616   19268 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0620 18:41:49.513901   19268 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0620 18:41:49.513901   19268 start.go:472] detecting cgroup driver to use...
I0620 18:41:49.513901   19268 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0620 18:41:49.515620   19268 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0620 18:41:49.537295   19268 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0620 18:41:49.552434   19268 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0620 18:41:49.564941   19268 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0620 18:41:49.568761   19268 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0620 18:41:49.585088   19268 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0620 18:41:49.600387   19268 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0620 18:41:49.615887   19268 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0620 18:41:49.631711   19268 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0620 18:41:49.646642   19268 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0620 18:41:49.678923   19268 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0620 18:41:49.712265   19268 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0620 18:41:49.744409   19268 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0620 18:41:49.886327   19268 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0620 18:41:50.108052   19268 start.go:472] detecting cgroup driver to use...
I0620 18:41:50.108052   19268 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0620 18:41:50.149585   19268 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0620 18:41:50.170047   19268 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0620 18:41:50.195505   19268 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0620 18:41:50.213047   19268 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0620 18:41:50.239624   19268 ssh_runner.go:195] Run: which cri-dockerd
I0620 18:41:50.272436   19268 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0620 18:41:50.299637   19268 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0620 18:41:50.344168   19268 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0620 18:41:50.553564   19268 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0620 18:41:50.687675   19268 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0620 18:41:50.688181   19268 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0620 18:41:50.748643   19268 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0620 18:41:50.912105   19268 ssh_runner.go:195] Run: sudo systemctl restart docker
I0620 18:41:51.465348   19268 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0620 18:41:51.599977   19268 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0620 18:41:51.764377   19268 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0620 18:41:51.881496   19268 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0620 18:41:52.015432   19268 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0620 18:41:52.055488   19268 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0620 18:41:52.168814   19268 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0620 18:41:52.451518   19268 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0620 18:41:52.456933   19268 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0620 18:41:52.464291   19268 start.go:540] Will wait 60s for crictl version
I0620 18:41:52.467902   19268 ssh_runner.go:195] Run: which crictl
I0620 18:41:52.500626   19268 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0620 18:41:52.684389   19268 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0620 18:41:52.697262   19268 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0620 18:41:52.808286   19268 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0620 18:41:52.843811   19268 out.go:204] * Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0620 18:41:52.859631   19268 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0620 18:41:53.121774   19268 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0620 18:41:53.125032   19268 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0620 18:41:53.130984   19268 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0620 18:41:53.157115   19268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0620 18:41:53.361927   19268 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0620 18:41:53.379116   19268 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0620 18:41:53.429905   19268 docker.go:671] Got preloaded images: -- stdout --
saotech/fast-api-app1:v1
saotech/frontend:v1
saotech/order-service:latest
redis:latest
nginx:<none>
nginx:latest
postgres:latest
postgres:<none>
nginx:<none>
httpd:2.4
httpd:latest
redis:<none>
dockersamples/examplevotingapp_vote:latest
nginx:1.25
saotech/react-appx1:v1
bitnami/apache:2.4.58-debian-12-r16
nginx:<none>
quay.io/argoproj/argocd:v2.10.1
bitnami/mariadb:<none>
bitnami/wordpress:6.4.3-debian-11-r4
bitnami/mariadb:11.2.2-debian-11-r6
redis:<none>
redis:7.0.14-alpine
saotech/web:v1
saotech/gottoimg:latest
saotech/koolimg:latest
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
ghcr.io/dexidp/dex:v2.37.0
registry.k8s.io/etcd:3.5.9-0
tccr.io/tccr/db-wait-mariadb:<none>
registry.k8s.io/coredns/coredns:v1.10.1
kodekloud/examplevotingapp_result:v1
kodekloud/examplevotingapp_vote:v1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kicbase/echo-server:1.0
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5
kodekloud/examplevotingapp_worker:v1
nginx:1.14.2
gcr.io/heptio-images/ks-guestbook-demo:0.2
gcr.io/heptio-images/ks-guestbook-demo:0.1

-- /stdout --
I0620 18:41:53.430466   19268 docker.go:601] Images already preloaded, skipping extraction
I0620 18:41:53.448157   19268 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0620 18:41:53.482920   19268 docker.go:671] Got preloaded images: -- stdout --
saotech/fast-api-app1:v1
saotech/frontend:v1
saotech/order-service:latest
redis:latest
nginx:<none>
nginx:latest
postgres:<none>
postgres:latest
nginx:<none>
httpd:2.4
httpd:latest
redis:<none>
dockersamples/examplevotingapp_vote:latest
nginx:1.25
saotech/react-appx1:v1
bitnami/apache:2.4.58-debian-12-r16
nginx:<none>
quay.io/argoproj/argocd:v2.10.1
bitnami/mariadb:<none>
bitnami/wordpress:6.4.3-debian-11-r4
bitnami/mariadb:11.2.2-debian-11-r6
redis:<none>
redis:7.0.14-alpine
saotech/web:v1
saotech/gottoimg:latest
saotech/koolimg:latest
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
ghcr.io/dexidp/dex:v2.37.0
registry.k8s.io/etcd:3.5.9-0
tccr.io/tccr/db-wait-mariadb:<none>
registry.k8s.io/coredns/coredns:v1.10.1
kodekloud/examplevotingapp_result:v1
kodekloud/examplevotingapp_vote:v1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kicbase/echo-server:1.0
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5
kodekloud/examplevotingapp_worker:v1
nginx:1.14.2
gcr.io/heptio-images/ks-guestbook-demo:0.2
gcr.io/heptio-images/ks-guestbook-demo:0.1

-- /stdout --
I0620 18:41:53.482920   19268 cache_images.go:84] Images are preloaded, skipping loading
I0620 18:41:53.495208   19268 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0620 18:41:53.733241   19268 cni.go:84] Creating CNI manager for ""
I0620 18:41:53.737053   19268 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0620 18:41:53.737053   19268 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0620 18:41:53.737613   19268 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0620 18:41:53.737613   19268 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0620 18:41:53.737613   19268 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0620 18:41:53.756400   19268 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0620 18:41:53.771142   19268 binaries.go:44] Found k8s binaries, skipping transfer
I0620 18:41:53.789225   19268 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0620 18:41:53.801266   19268 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0620 18:41:53.820669   19268 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0620 18:41:53.842142   19268 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0620 18:41:53.866672   19268 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0620 18:41:53.873716   19268 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0620 18:41:53.887559   19268 certs.go:56] Setting up C:\Users\omerm\.minikube\profiles\minikube for IP: 192.168.49.2
I0620 18:41:53.888067   19268 certs.go:190] acquiring lock for shared ca certs: {Name:mkf9e95f47d90690dd1ff232bf829cd3e57329a3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0620 18:41:53.903605   19268 certs.go:199] skipping minikubeCA CA generation: C:\Users\omerm\.minikube\ca.key
I0620 18:41:53.922550   19268 certs.go:199] skipping proxyClientCA CA generation: C:\Users\omerm\.minikube\proxy-client-ca.key
I0620 18:41:53.923666   19268 certs.go:315] skipping minikube-user signed cert generation: C:\Users\omerm\.minikube\profiles\minikube\client.key
I0620 18:41:53.939982   19268 certs.go:315] skipping minikube signed cert generation: C:\Users\omerm\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I0620 18:41:53.957712   19268 certs.go:315] skipping aggregator signed cert generation: C:\Users\omerm\.minikube\profiles\minikube\proxy-client.key
I0620 18:41:53.960860   19268 certs.go:437] found cert: C:\Users\omerm\.minikube\certs\C:\Users\omerm\.minikube\certs\ca-key.pem (1679 bytes)
I0620 18:41:53.960860   19268 certs.go:437] found cert: C:\Users\omerm\.minikube\certs\C:\Users\omerm\.minikube\certs\ca.pem (1074 bytes)
I0620 18:41:53.960860   19268 certs.go:437] found cert: C:\Users\omerm\.minikube\certs\C:\Users\omerm\.minikube\certs\cert.pem (1119 bytes)
I0620 18:41:53.960860   19268 certs.go:437] found cert: C:\Users\omerm\.minikube\certs\C:\Users\omerm\.minikube\certs\key.pem (1675 bytes)
I0620 18:41:53.960860   19268 ssh_runner.go:362] scp C:\Users\omerm\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0620 18:41:53.992241   19268 ssh_runner.go:362] scp C:\Users\omerm\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0620 18:41:54.020330   19268 ssh_runner.go:362] scp C:\Users\omerm\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0620 18:41:54.048502   19268 ssh_runner.go:362] scp C:\Users\omerm\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0620 18:41:54.077489   19268 ssh_runner.go:362] scp C:\Users\omerm\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0620 18:41:54.108934   19268 ssh_runner.go:362] scp C:\Users\omerm\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0620 18:41:54.138203   19268 ssh_runner.go:362] scp C:\Users\omerm\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0620 18:41:54.169240   19268 ssh_runner.go:362] scp C:\Users\omerm\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0620 18:41:54.198784   19268 ssh_runner.go:362] scp C:\Users\omerm\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0620 18:41:54.226841   19268 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0620 18:41:54.250041   19268 ssh_runner.go:195] Run: openssl version
I0620 18:41:54.282536   19268 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0620 18:41:54.299520   19268 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0620 18:41:54.308075   19268 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Dec 26  2023 /usr/share/ca-certificates/minikubeCA.pem
I0620 18:41:54.309859   19268 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0620 18:41:54.338751   19268 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0620 18:41:54.357026   19268 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0620 18:41:54.369453   19268 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0620 18:41:54.382518   19268 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0620 18:41:54.397427   19268 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0620 18:41:54.410601   19268 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0620 18:41:54.423139   19268 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0620 18:41:54.435652   19268 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0620 18:41:54.446716   19268 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\omerm:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0620 18:41:54.460378   19268 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0620 18:41:54.510307   19268 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0620 18:41:54.522815   19268 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0620 18:41:54.522815   19268 kubeadm.go:636] restartCluster start
I0620 18:41:54.542727   19268 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0620 18:41:54.556113   19268 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0620 18:41:54.567829   19268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0620 18:41:54.724547   19268 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:31657"
I0620 18:41:54.724547   19268 kubeconfig.go:135] verify returned: got: 127.0.0.1:31657, want: 127.0.0.1:27280
I0620 18:41:54.736686   19268 lock.go:35] WriteFile acquiring C:\Users\omerm\.kube\config: {Name:mk8984666553b4f68983dc8f652f452a830a0d3b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0620 18:41:54.783315   19268 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0620 18:41:54.795902   19268 api_server.go:166] Checking apiserver status ...
I0620 18:41:54.812720   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:41:54.825904   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:41:54.825904   19268 api_server.go:166] Checking apiserver status ...
I0620 18:41:54.846259   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:41:54.858815   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:41:55.372356   19268 api_server.go:166] Checking apiserver status ...
I0620 18:41:55.391733   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:41:55.405908   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:41:55.862554   19268 api_server.go:166] Checking apiserver status ...
I0620 18:41:55.924583   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:41:55.937647   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:41:56.368512   19268 api_server.go:166] Checking apiserver status ...
I0620 18:41:56.399869   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:41:56.413541   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:41:56.860134   19268 api_server.go:166] Checking apiserver status ...
I0620 18:41:56.884942   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:41:56.900013   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:41:57.367965   19268 api_server.go:166] Checking apiserver status ...
I0620 18:41:57.386867   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:41:57.402226   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:41:57.859925   19268 api_server.go:166] Checking apiserver status ...
I0620 18:41:57.889517   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:41:57.905121   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:41:58.367985   19268 api_server.go:166] Checking apiserver status ...
I0620 18:41:58.426665   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:41:58.439537   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:41:58.861744   19268 api_server.go:166] Checking apiserver status ...
I0620 18:41:58.889698   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:41:58.902628   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:41:59.368441   19268 api_server.go:166] Checking apiserver status ...
I0620 18:41:59.388843   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:41:59.401364   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:41:59.862547   19268 api_server.go:166] Checking apiserver status ...
I0620 18:41:59.914377   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:41:59.928341   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:42:00.364428   19268 api_server.go:166] Checking apiserver status ...
I0620 18:42:00.406675   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:42:00.422982   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:42:00.866098   19268 api_server.go:166] Checking apiserver status ...
I0620 18:42:00.898381   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:42:00.911532   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:42:01.359505   19268 api_server.go:166] Checking apiserver status ...
I0620 18:42:01.404391   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:42:01.418254   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:42:01.868955   19268 api_server.go:166] Checking apiserver status ...
I0620 18:42:01.889649   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:42:01.903674   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:42:02.361777   19268 api_server.go:166] Checking apiserver status ...
I0620 18:42:02.396970   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:42:02.413588   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:42:02.865162   19268 api_server.go:166] Checking apiserver status ...
I0620 18:42:02.892356   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:42:02.905016   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:42:03.372559   19268 api_server.go:166] Checking apiserver status ...
I0620 18:42:03.396104   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:42:03.412770   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:42:03.862271   19268 api_server.go:166] Checking apiserver status ...
I0620 18:42:03.882913   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:42:03.897316   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:42:04.370303   19268 api_server.go:166] Checking apiserver status ...
I0620 18:42:04.397714   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 18:42:04.409878   19268 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0620 18:42:04.806441   19268 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0620 18:42:04.806441   19268 kubeadm.go:1128] stopping kube-system containers ...
I0620 18:42:04.822635   19268 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0620 18:42:04.859161   19268 docker.go:469] Stopping containers: [12b1fddfdc66 da2c7c1aa2d2 6838dd5e06ad a3a64b87f4f3 0bb4b548ac78 ad726e855a8b b84b658b712f 5dbcf03e87c1 1404c722267d 3e62202256d3 d478b5616255 c655f9af2ffe 96ba22ff689d c292efab8d4b d6066c9ec529 6a3d84894803 9983290fb1b5 1c80df54b8ff 4aed169eb736 17c7a0ca3154 979db965571a f7afcfee50c5 51b033771cc3 21be80e0d970 d7f00976ae4a 7104b920dcee aa6a281e3890]
I0620 18:42:04.874019   19268 ssh_runner.go:195] Run: docker stop 12b1fddfdc66 da2c7c1aa2d2 6838dd5e06ad a3a64b87f4f3 0bb4b548ac78 ad726e855a8b b84b658b712f 5dbcf03e87c1 1404c722267d 3e62202256d3 d478b5616255 c655f9af2ffe 96ba22ff689d c292efab8d4b d6066c9ec529 6a3d84894803 9983290fb1b5 1c80df54b8ff 4aed169eb736 17c7a0ca3154 979db965571a f7afcfee50c5 51b033771cc3 21be80e0d970 d7f00976ae4a 7104b920dcee aa6a281e3890
I0620 18:42:04.925164   19268 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0620 18:42:04.957298   19268 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0620 18:42:04.970359   19268 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Apr 15 11:26 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Jun 19 12:35 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Apr 15 11:27 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Jun 19 12:35 /etc/kubernetes/scheduler.conf

I0620 18:42:04.987395   19268 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0620 18:42:05.032867   19268 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0620 18:42:05.064869   19268 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0620 18:42:05.080473   19268 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0620 18:42:05.101898   19268 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0620 18:42:05.138772   19268 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0620 18:42:05.154833   19268 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0620 18:42:05.173636   19268 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0620 18:42:05.205213   19268 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0620 18:42:05.218658   19268 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0620 18:42:05.218658   19268 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0620 18:42:05.436449   19268 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0620 18:42:06.456487   19268 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.0194563s)
I0620 18:42:06.456487   19268 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0620 18:42:06.690349   19268 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0620 18:42:06.759113   19268 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0620 18:42:06.826059   19268 api_server.go:52] waiting for apiserver process to appear ...
I0620 18:42:06.853023   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0620 18:42:06.914228   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0620 18:42:07.458807   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0620 18:42:07.973827   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0620 18:42:08.461610   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0620 18:42:08.962577   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0620 18:42:08.992263   19268 api_server.go:72] duration metric: took 2.1662039s to wait for apiserver process to appear ...
I0620 18:42:08.992263   19268 api_server.go:88] waiting for apiserver healthz status ...
I0620 18:42:08.992836   19268 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:27280/healthz ...
I0620 18:42:08.996161   19268 api_server.go:269] stopped: https://127.0.0.1:27280/healthz: Get "https://127.0.0.1:27280/healthz": EOF
I0620 18:42:08.996161   19268 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:27280/healthz ...
I0620 18:42:09.000193   19268 api_server.go:269] stopped: https://127.0.0.1:27280/healthz: Get "https://127.0.0.1:27280/healthz": EOF
I0620 18:42:09.500222   19268 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:27280/healthz ...
I0620 18:42:09.502502   19268 api_server.go:269] stopped: https://127.0.0.1:27280/healthz: Get "https://127.0.0.1:27280/healthz": EOF
I0620 18:42:10.001798   19268 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:27280/healthz ...
I0620 18:42:13.610398   19268 api_server.go:279] https://127.0.0.1:27280/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0620 18:42:13.610398   19268 api_server.go:103] status: https://127.0.0.1:27280/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0620 18:42:13.610398   19268 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:27280/healthz ...
I0620 18:42:13.888992   19268 api_server.go:279] https://127.0.0.1:27280/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0620 18:42:13.888992   19268 api_server.go:103] status: https://127.0.0.1:27280/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0620 18:42:14.005403   19268 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:27280/healthz ...
I0620 18:42:14.016987   19268 api_server.go:279] https://127.0.0.1:27280/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0620 18:42:14.016987   19268 api_server.go:103] status: https://127.0.0.1:27280/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0620 18:42:14.513300   19268 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:27280/healthz ...
I0620 18:42:14.534798   19268 api_server.go:279] https://127.0.0.1:27280/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0620 18:42:14.534798   19268 api_server.go:103] status: https://127.0.0.1:27280/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0620 18:42:15.003635   19268 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:27280/healthz ...
I0620 18:42:15.016671   19268 api_server.go:279] https://127.0.0.1:27280/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0620 18:42:15.016671   19268 api_server.go:103] status: https://127.0.0.1:27280/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0620 18:42:15.510497   19268 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:27280/healthz ...
I0620 18:42:15.598537   19268 api_server.go:279] https://127.0.0.1:27280/healthz returned 200:
ok
I0620 18:42:15.793073   19268 api_server.go:141] control plane version: v1.28.3
I0620 18:42:15.793073   19268 api_server.go:131] duration metric: took 6.8008101s to wait for apiserver health ...
I0620 18:42:15.793073   19268 cni.go:84] Creating CNI manager for ""
I0620 18:42:15.793073   19268 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0620 18:42:15.798072   19268 out.go:177] * Configuring bridge CNI (Container Networking Interface) ...
I0620 18:42:15.862097   19268 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0620 18:42:15.899839   19268 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0620 18:42:16.185785   19268 system_pods.go:43] waiting for kube-system pods to appear ...
I0620 18:42:16.216243   19268 system_pods.go:59] 7 kube-system pods found
I0620 18:42:16.216243   19268 system_pods.go:61] "coredns-5dd5756b68-kczsx" [4d4280ef-48f1-454d-bef3-859e767af4b0] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0620 18:42:16.216243   19268 system_pods.go:61] "etcd-minikube" [58013141-f91e-4bf4-bf37-fcd6e1818fe9] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0620 18:42:16.216243   19268 system_pods.go:61] "kube-apiserver-minikube" [729fe257-45a4-4ec8-82c5-eaa9acd5d8b5] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0620 18:42:16.216243   19268 system_pods.go:61] "kube-controller-manager-minikube" [9b8a9d03-f4aa-44a4-959b-161826932d3c] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0620 18:42:16.216243   19268 system_pods.go:61] "kube-proxy-t56xn" [a22c4b07-f55f-4565-8fbd-d93efaea4e58] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0620 18:42:16.216243   19268 system_pods.go:61] "kube-scheduler-minikube" [1eaba658-d157-46ce-9c21-d31ccdd62a43] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0620 18:42:16.216243   19268 system_pods.go:61] "storage-provisioner" [c094f5b4-236b-4fbf-a063-a5597db35482] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0620 18:42:16.216243   19268 system_pods.go:74] duration metric: took 30.4578ms to wait for pod list to return data ...
I0620 18:42:16.216243   19268 node_conditions.go:102] verifying NodePressure condition ...
I0620 18:42:16.288981   19268 node_conditions.go:122] node storage ephemeral capacity is 263112772Ki
I0620 18:42:16.288981   19268 node_conditions.go:123] node cpu capacity is 16
I0620 18:42:16.289536   19268 node_conditions.go:105] duration metric: took 73.2927ms to run NodePressure ...
I0620 18:42:16.289536   19268 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0620 18:42:19.095768   19268 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (2.8062326s)
I0620 18:42:19.095768   19268 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0620 18:42:19.298277   19268 ops.go:34] apiserver oom_adj: -16
I0620 18:42:19.298277   19268 kubeadm.go:640] restartCluster took 24.7754618s
I0620 18:42:19.298277   19268 kubeadm.go:406] StartCluster complete in 24.8515609s
I0620 18:42:19.298938   19268 settings.go:142] acquiring lock: {Name:mk25c9d3e88c9f5fca9ed4604bcf43b335d7e8b8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0620 18:42:19.299565   19268 settings.go:150] Updating kubeconfig:  C:\Users\omerm\.kube\config
I0620 18:42:19.307595   19268 lock.go:35] WriteFile acquiring C:\Users\omerm\.kube\config: {Name:mk8984666553b4f68983dc8f652f452a830a0d3b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0620 18:42:19.311832   19268 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0620 18:42:19.311832   19268 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0620 18:42:19.312427   19268 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0620 18:42:19.312427   19268 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0620 18:42:19.312427   19268 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0620 18:42:19.312427   19268 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0620 18:42:19.312427   19268 addons.go:240] addon storage-provisioner should already be in state true
I0620 18:42:19.312427   19268 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0620 18:42:19.312427   19268 addons.go:69] Setting ingress=true in profile "minikube"
I0620 18:42:19.312427   19268 addons.go:231] Setting addon ingress=true in "minikube"
W0620 18:42:19.312427   19268 addons.go:240] addon ingress should already be in state true
I0620 18:42:19.313051   19268 addons.go:69] Setting dashboard=true in profile "minikube"
I0620 18:42:19.313051   19268 addons.go:231] Setting addon dashboard=true in "minikube"
W0620 18:42:19.313051   19268 addons.go:240] addon dashboard should already be in state true
I0620 18:42:19.316233   19268 host.go:66] Checking if "minikube" exists ...
I0620 18:42:19.316233   19268 host.go:66] Checking if "minikube" exists ...
I0620 18:42:19.316233   19268 host.go:66] Checking if "minikube" exists ...
I0620 18:42:19.403037   19268 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0620 18:42:19.403711   19268 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0620 18:42:19.407784   19268 out.go:177] * Verifying Kubernetes components...
I0620 18:42:19.449798   19268 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0620 18:42:19.453466   19268 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0620 18:42:19.456891   19268 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0620 18:42:19.458625   19268 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0620 18:42:19.471128   19268 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0620 18:42:19.666242   19268 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0620 18:42:19.668257   19268 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0620 18:42:19.668257   19268 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0620 18:42:19.668855   19268 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0620 18:42:19.668855   19268 addons.go:240] addon default-storageclass should already be in state true
I0620 18:42:19.669452   19268 host.go:66] Checking if "minikube" exists ...
I0620 18:42:19.681365   19268 out.go:177]   - Using image docker.io/kubernetesui/dashboard:v2.7.0
I0620 18:42:19.682448   19268 out.go:177] * After the addon is enabled, please run "minikube tunnel" and your ingress resources would be available at "127.0.0.1"
I0620 18:42:19.684119   19268 out.go:177]   - Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0620 18:42:19.685279   19268 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0620 18:42:19.685279   19268 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0620 18:42:19.684119   19268 out.go:177]   - Using image registry.k8s.io/ingress-nginx/controller:v1.9.4
I0620 18:42:19.685279   19268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 18:42:19.687559   19268 out.go:177]   - Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0
I0620 18:42:19.689540   19268 out.go:177]   - Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0
I0620 18:42:19.691324   19268 addons.go:423] installing /etc/kubernetes/addons/ingress-deploy.yaml
I0620 18:42:19.691324   19268 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/ingress-deploy.yaml (16103 bytes)
I0620 18:42:19.722567   19268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 18:42:19.724824   19268 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0620 18:42:19.725902   19268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 18:42:19.927723   19268 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:27276 SSHKeyPath:C:\Users\omerm\.minikube\machines\minikube\id_rsa Username:docker}
I0620 18:42:19.943254   19268 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:27276 SSHKeyPath:C:\Users\omerm\.minikube\machines\minikube\id_rsa Username:docker}
I0620 18:42:19.958972   19268 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0620 18:42:19.958972   19268 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0620 18:42:19.972617   19268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 18:42:19.974432   19268 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:27276 SSHKeyPath:C:\Users\omerm\.minikube\machines\minikube\id_rsa Username:docker}
I0620 18:42:20.177272   19268 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:27276 SSHKeyPath:C:\Users\omerm\.minikube\machines\minikube\id_rsa Username:docker}
I0620 18:42:20.841525   19268 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0620 18:42:20.891344   19268 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0620 18:42:20.891931   19268 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0620 18:42:20.933311   19268 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0620 18:42:20.936794   19268 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0620 18:42:21.094971   19268 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0620 18:42:21.094971   19268 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0620 18:42:21.502591   19268 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0620 18:42:21.502591   19268 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0620 18:42:22.102075   19268 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0620 18:42:22.102075   19268 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0620 18:42:22.404081   19268 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I0620 18:42:22.404081   19268 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0620 18:42:22.798225   19268 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0620 18:42:22.798225   19268 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0620 18:42:23.000708   19268 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (3.6888753s)
I0620 18:42:23.001356   19268 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (3.5295797s)
I0620 18:42:23.001356   19268 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0620 18:42:23.046272   19268 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0620 18:42:23.095402   19268 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0620 18:42:23.095402   19268 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0620 18:42:23.206234   19268 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0620 18:42:23.206234   19268 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0620 18:42:23.295969   19268 api_server.go:52] waiting for apiserver process to appear ...
I0620 18:42:23.329589   19268 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0620 18:42:23.391201   19268 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0620 18:42:23.391201   19268 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0620 18:42:23.524351   19268 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0620 18:42:27.354068   19268 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: (6.5125432s)
I0620 18:42:27.354068   19268 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (6.4172742s)
I0620 18:42:27.354068   19268 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (6.4207567s)
I0620 18:42:27.354678   19268 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (4.0244791s)
I0620 18:42:27.354678   19268 api_server.go:72] duration metric: took 7.9509669s to wait for apiserver process to appear ...
I0620 18:42:27.354678   19268 api_server.go:88] waiting for apiserver healthz status ...
I0620 18:42:27.354678   19268 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:27280/healthz ...
I0620 18:42:27.354678   19268 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (3.8303266s)
I0620 18:42:27.356988   19268 addons.go:467] Verifying addon ingress=true in "minikube"
I0620 18:42:27.358189   19268 out.go:177] * Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I0620 18:42:27.359803   19268 out.go:177] * Verifying ingress addon...
I0620 18:42:27.364419   19268 kapi.go:75] Waiting for pod with label "app.kubernetes.io/name=ingress-nginx" in ns "ingress-nginx" ...
I0620 18:42:27.364419   19268 api_server.go:279] https://127.0.0.1:27280/healthz returned 200:
ok
I0620 18:42:27.367498   19268 api_server.go:141] control plane version: v1.28.3
I0620 18:42:27.367498   19268 api_server.go:131] duration metric: took 12.8203ms to wait for apiserver health ...
I0620 18:42:27.367498   19268 system_pods.go:43] waiting for kube-system pods to appear ...
I0620 18:42:27.371344   19268 kapi.go:86] Found 3 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0620 18:42:27.371344   19268 kapi.go:107] duration metric: took 6.9254ms to wait for app.kubernetes.io/name=ingress-nginx ...
I0620 18:42:27.374220   19268 out.go:177] * Enabled addons: storage-provisioner, dashboard, ingress, default-storageclass
I0620 18:42:27.375365   19268 addons.go:502] enable addons completed in 8.0640993s: enabled=[storage-provisioner dashboard ingress default-storageclass]
I0620 18:42:27.375918   19268 system_pods.go:59] 7 kube-system pods found
I0620 18:42:27.375918   19268 system_pods.go:61] "coredns-5dd5756b68-kczsx" [4d4280ef-48f1-454d-bef3-859e767af4b0] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0620 18:42:27.375918   19268 system_pods.go:61] "etcd-minikube" [58013141-f91e-4bf4-bf37-fcd6e1818fe9] Running
I0620 18:42:27.375918   19268 system_pods.go:61] "kube-apiserver-minikube" [729fe257-45a4-4ec8-82c5-eaa9acd5d8b5] Running
I0620 18:42:27.375918   19268 system_pods.go:61] "kube-controller-manager-minikube" [9b8a9d03-f4aa-44a4-959b-161826932d3c] Running
I0620 18:42:27.375918   19268 system_pods.go:61] "kube-proxy-t56xn" [a22c4b07-f55f-4565-8fbd-d93efaea4e58] Running
I0620 18:42:27.375918   19268 system_pods.go:61] "kube-scheduler-minikube" [1eaba658-d157-46ce-9c21-d31ccdd62a43] Running
I0620 18:42:27.375918   19268 system_pods.go:61] "storage-provisioner" [c094f5b4-236b-4fbf-a063-a5597db35482] Running
I0620 18:42:27.375918   19268 system_pods.go:74] duration metric: took 8.4193ms to wait for pod list to return data ...
I0620 18:42:27.375918   19268 kubeadm.go:581] duration metric: took 7.9722065s to wait for : map[apiserver:true system_pods:true] ...
I0620 18:42:27.375918   19268 node_conditions.go:102] verifying NodePressure condition ...
I0620 18:42:27.380258   19268 node_conditions.go:122] node storage ephemeral capacity is 263112772Ki
I0620 18:42:27.380908   19268 node_conditions.go:123] node cpu capacity is 16
I0620 18:42:27.380908   19268 node_conditions.go:105] duration metric: took 4.9908ms to run NodePressure ...
I0620 18:42:27.380908   19268 start.go:228] waiting for startup goroutines ...
I0620 18:42:27.380908   19268 start.go:233] waiting for cluster config update ...
I0620 18:42:27.380908   19268 start.go:242] writing updated cluster config ...
I0620 18:42:27.384618   19268 ssh_runner.go:195] Run: rm -f paused
I0620 18:42:27.519037   19268 start.go:600] kubectl: 1.28.2, cluster: 1.28.3 (minor skew: 0)
I0620 18:42:27.521528   19268 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Jun 20 16:17:29 minikube dockerd[1005]: time="2025-06-20T16:17:29.195207148Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 20 16:21:08 minikube dockerd[1005]: time="2025-06-20T16:21:08.715470387Z" level=info msg="ignoring event" container=3b4c6eb03ad0bfa813c17d3b37bad898f2c8e2f867d0a1a6e169419e086ec470 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 16:21:09 minikube cri-dockerd[1258]: time="2025-06-20T16:21:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9a241b28eedc8dd9cc63e38dd7ea3e6a7580cc40848c13f8eba043d9ea4a7293/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 20 16:21:14 minikube dockerd[1005]: time="2025-06-20T16:21:14.325020757Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jun 20 16:21:14 minikube dockerd[1005]: time="2025-06-20T16:21:14.325077661Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 20 16:21:19 minikube dockerd[1005]: time="2025-06-20T16:21:19.045512986Z" level=info msg="ignoring event" container=9448e847a2167823ebc378e1deb19770123b8630f9c2f7a68c1faa0a90df6009 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 16:21:19 minikube cri-dockerd[1258]: time="2025-06-20T16:21:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7c6615d762346fe7a97ffa9aa23832c384ae9239838dbad61a8726522cff0fad/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 20 16:21:27 minikube cri-dockerd[1258]: time="2025-06-20T16:21:27Z" level=info msg="Stop pulling image saotech/k8s_frontend:78726673b1b4ab4be5a73a552b577144e9d05479: Status: Downloaded newer image for saotech/k8s_frontend:78726673b1b4ab4be5a73a552b577144e9d05479"
Jun 20 16:21:30 minikube dockerd[1005]: time="2025-06-20T16:21:30.397556918Z" level=info msg="ignoring event" container=bcc36dff7df46f0a67add9add5727c6db1b0d3815051b82baf3dd4017d8baf1d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 16:21:30 minikube dockerd[1005]: time="2025-06-20T16:21:30.839467655Z" level=info msg="ignoring event" container=4aab739b176f8c92ae3a0b00c274f7928e15d759091d1bd05e6754f18e2b4e8b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 16:21:34 minikube dockerd[1005]: time="2025-06-20T16:21:34.422105583Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jun 20 16:21:34 minikube dockerd[1005]: time="2025-06-20T16:21:34.422164884Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 20 16:22:05 minikube dockerd[1005]: time="2025-06-20T16:22:05.075449131Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jun 20 16:22:05 minikube dockerd[1005]: time="2025-06-20T16:22:05.075563832Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 20 16:22:58 minikube dockerd[1005]: time="2025-06-20T16:22:58.463518717Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jun 20 16:22:58 minikube dockerd[1005]: time="2025-06-20T16:22:58.463699019Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 20 16:24:36 minikube dockerd[1005]: time="2025-06-20T16:24:36.461100267Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jun 20 16:24:36 minikube dockerd[1005]: time="2025-06-20T16:24:36.461268066Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 20 16:24:58 minikube cri-dockerd[1258]: time="2025-06-20T16:24:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ccbe8ed350805954c2b231dbf476661ceb8acd667d9b05e55ce6a802876443fe/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 20 16:25:07 minikube cri-dockerd[1258]: time="2025-06-20T16:25:07Z" level=info msg="Stop pulling image saotech/k8s_frontend:9ff1c2ae9e2608ee93e4a0c6d58b436cfc7cfebd: Status: Downloaded newer image for saotech/k8s_frontend:9ff1c2ae9e2608ee93e4a0c6d58b436cfc7cfebd"
Jun 20 16:25:08 minikube dockerd[1005]: time="2025-06-20T16:25:08.678235286Z" level=info msg="ignoring event" container=f26c0f6a378ad957f8e97c7f21de1baae8edcaee1bac317e72b521ca497ba386 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 16:25:08 minikube dockerd[1005]: time="2025-06-20T16:25:08.955516080Z" level=info msg="ignoring event" container=7c6615d762346fe7a97ffa9aa23832c384ae9239838dbad61a8726522cff0fad module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 17:41:25 minikube dockerd[1005]: time="2025-06-20T17:41:25.271703198Z" level=info msg="ignoring event" container=9a241b28eedc8dd9cc63e38dd7ea3e6a7580cc40848c13f8eba043d9ea4a7293 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 17:41:25 minikube cri-dockerd[1258]: time="2025-06-20T17:41:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bb1e2eba718cf23f26d5747ecdc3e9b28583d25ff6452c40e347d51ef8abbdd9/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 20 17:41:36 minikube cri-dockerd[1258]: time="2025-06-20T17:41:36Z" level=info msg="Stop pulling image saotech/k8s_backend:9ff1c2ae9e2608ee93e4a0c6d58b436cfc7cfebd: Status: Downloaded newer image for saotech/k8s_backend:9ff1c2ae9e2608ee93e4a0c6d58b436cfc7cfebd"
Jun 20 17:41:38 minikube dockerd[1005]: time="2025-06-20T17:41:38.476397929Z" level=info msg="ignoring event" container=9e998151737c763146b29ff926cff00d692cfecc24a7618d0499d0baf07654b7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 17:41:38 minikube dockerd[1005]: time="2025-06-20T17:41:38.827905746Z" level=info msg="ignoring event" container=f4447806b2d138f181a3cd99adc896392814e58016d8f6fa85ff43e490833c85 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 17:46:07 minikube cri-dockerd[1258]: time="2025-06-20T17:46:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/49e65cd129ebc5ca6c8f59709de6748082a5f8279dc4e4df3aa62cdea79048dc/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 20 17:46:17 minikube cri-dockerd[1258]: time="2025-06-20T17:46:17Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.4: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.4"
Jun 20 17:46:17 minikube dockerd[1005]: time="2025-06-20T17:46:17.838227085Z" level=info msg="ignoring event" container=13ac5242718c1c9255581ead5f6f0ab6541765ad71c8c8bf3907e901772b39c5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 17:46:19 minikube dockerd[1005]: time="2025-06-20T17:46:19.255248647Z" level=info msg="ignoring event" container=49e65cd129ebc5ca6c8f59709de6748082a5f8279dc4e4df3aa62cdea79048dc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 18:02:58 minikube cri-dockerd[1258]: time="2025-06-20T18:02:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/47506b3dfe75236e9a98fb527de332f8ef8d37fb6c339e0f9f6415173689615d/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 20 18:02:58 minikube dockerd[1005]: time="2025-06-20T18:02:58.709396002Z" level=info msg="ignoring event" container=588545f0616e3647b2203c967b6de4964a760366a3bc26d8ac2c541d0aad6388 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 18:03:00 minikube dockerd[1005]: time="2025-06-20T18:03:00.195565869Z" level=info msg="ignoring event" container=47506b3dfe75236e9a98fb527de332f8ef8d37fb6c339e0f9f6415173689615d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 18:03:03 minikube dockerd[1005]: time="2025-06-20T18:03:03.226314738Z" level=warning msg="Published ports are discarded when using host network mode"
Jun 20 18:03:03 minikube dockerd[1005]: time="2025-06-20T18:03:03.493019407Z" level=warning msg="Published ports are discarded when using host network mode"
Jun 20 18:03:05 minikube cri-dockerd[1258]: time="2025-06-20T18:03:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d04ad9dd62f1ddaea3da3b8b06ca93c085c9261aa86853a0813fd2c13a2664fa/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 20 18:03:06 minikube cri-dockerd[1258]: time="2025-06-20T18:03:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5053c2339ac69a3a2ace0ea2855edcf16d264e733dca32b5c4222c15bddfc7c8/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 20 18:03:06 minikube cri-dockerd[1258]: time="2025-06-20T18:03:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/de43b8f5ee6510dcebf6ebd238b93a5b73e54cab929d86ff1cbe7335d02f66ec/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 20 18:03:07 minikube cri-dockerd[1258]: time="2025-06-20T18:03:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8ce98cef767e823cf043f7cac97806b72fa3fd6154e0f5fb6565dc010983cdd6/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 20 18:03:15 minikube cri-dockerd[1258]: time="2025-06-20T18:03:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/307b19ca96ca642e59358ca11817ba826a9fe2c7bb125b3409bbe2102577919e/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 20 18:03:15 minikube dockerd[1005]: time="2025-06-20T18:03:15.564186120Z" level=info msg="ignoring event" container=2b01572ac02e3f4f650636cb1daac7638f0f161d3b15b484f61a9a9f0724a9dd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 18:03:17 minikube dockerd[1005]: time="2025-06-20T18:03:17.454336955Z" level=info msg="ignoring event" container=307b19ca96ca642e59358ca11817ba826a9fe2c7bb125b3409bbe2102577919e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 18:03:20 minikube cri-dockerd[1258]: time="2025-06-20T18:03:20Z" level=info msg="Pulling image quay.io/prometheus/node-exporter:v1.9.1: 1617e25568b2: Pull complete "
Jun 20 18:03:24 minikube cri-dockerd[1258]: time="2025-06-20T18:03:24Z" level=info msg="Stop pulling image quay.io/prometheus/node-exporter:v1.9.1: Status: Downloaded newer image for quay.io/prometheus/node-exporter:v1.9.1"
Jun 20 18:03:32 minikube cri-dockerd[1258]: time="2025-06-20T18:03:32Z" level=info msg="Stop pulling image registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.15.0: Status: Downloaded newer image for registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.15.0"
Jun 20 18:03:39 minikube cri-dockerd[1258]: time="2025-06-20T18:03:39Z" level=error msg="Error response from daemon: No such container: 588545f0616e3647b2203c967b6de4964a760366a3bc26d8ac2c541d0aad6388 Failed to get stats from container 588545f0616e3647b2203c967b6de4964a760366a3bc26d8ac2c541d0aad6388"
Jun 20 18:03:39 minikube cri-dockerd[1258]: time="2025-06-20T18:03:39Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"monitoring-kube-prometheus-admission-patch-rchvz_monitoring\": CNI failed to retrieve network namespace path: Error response from daemon: No such container: 307b19ca96ca642e59358ca11817ba826a9fe2c7bb125b3409bbe2102577919e"
Jun 20 18:03:42 minikube cri-dockerd[1258]: time="2025-06-20T18:03:42Z" level=info msg="Stop pulling image quay.io/prometheus-operator/prometheus-operator:v0.83.0: Status: Downloaded newer image for quay.io/prometheus-operator/prometheus-operator:v0.83.0"
Jun 20 18:03:48 minikube cri-dockerd[1258]: time="2025-06-20T18:03:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0aca40a0e69c498cd3c813676daffc43d466b9f8d8a546887a2059f6d5d2a897/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 20 18:03:48 minikube cri-dockerd[1258]: time="2025-06-20T18:03:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/79c513d9bb12067bb7c0a4c68f524e34fbc0c16142f7b73986f3e9ab692db217/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 20 18:03:51 minikube cri-dockerd[1258]: time="2025-06-20T18:03:51Z" level=info msg="Stop pulling image docker.io/curlimages/curl:8.9.1: Status: Downloaded newer image for curlimages/curl:8.9.1"
Jun 20 18:03:51 minikube dockerd[1005]: time="2025-06-20T18:03:51.686720103Z" level=info msg="ignoring event" container=702eaa0c1436de94151a05257646ad6d06c1ba3a86ba3b430fe69751ff0d0023 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 18:04:00 minikube cri-dockerd[1258]: time="2025-06-20T18:04:00Z" level=info msg="Stop pulling image quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0: Status: Downloaded newer image for quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0"
Jun 20 18:04:00 minikube dockerd[1005]: time="2025-06-20T18:04:00.560874313Z" level=info msg="ignoring event" container=ccecf6587ac2c30082186c01be2bf6b067461c238816397dc00477ad8895e9c2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 18:04:03 minikube cri-dockerd[1258]: time="2025-06-20T18:04:03Z" level=info msg="Stop pulling image quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0: Status: Image is up to date for quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0"
Jun 20 18:04:03 minikube dockerd[1005]: time="2025-06-20T18:04:03.854422228Z" level=info msg="ignoring event" container=784fc6cc477ae56edce6b8aea036c0099897ab3f46625c056928b2772b20c9a0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 18:04:17 minikube cri-dockerd[1258]: time="2025-06-20T18:04:17Z" level=info msg="Pulling image quay.io/kiwigrid/k8s-sidecar:1.30.0: 72f47f2155f6: Extracting [============================>                      ]  3.342MB/5.879MB"
Jun 20 18:04:18 minikube cri-dockerd[1258]: time="2025-06-20T18:04:18Z" level=info msg="Stop pulling image quay.io/kiwigrid/k8s-sidecar:1.30.0: Status: Downloaded newer image for quay.io/kiwigrid/k8s-sidecar:1.30.0"
Jun 20 18:04:30 minikube cri-dockerd[1258]: time="2025-06-20T18:04:30Z" level=info msg="Stop pulling image quay.io/prometheus/alertmanager:v0.28.1: Status: Downloaded newer image for quay.io/prometheus/alertmanager:v0.28.1"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                                            CREATED              STATE               NAME                               ATTEMPT             POD ID              POD
04658960b61e0       quay.io/prometheus/alertmanager@sha256:27c475db5fb156cab31d5c18a4251ac7ed567746a2483ff264516437a39b15ba                          6 seconds ago        Running             alertmanager                       0                   0aca40a0e69c4       alertmanager-monitoring-kube-prometheus-alertmanager-0
c952b8ba8303b       0d812acc1ba03                                                                                                                    6 seconds ago        Running             config-reloader                    0                   0aca40a0e69c4       alertmanager-monitoring-kube-prometheus-alertmanager-0
395431b5bb91b       2b559fb10a9f8                                                                                                                    17 seconds ago       Running             grafana-sc-datasources             0                   8ce98cef767e8       monitoring-grafana-68567f8765-gd2tt
a5140f7eec094       quay.io/kiwigrid/k8s-sidecar@sha256:9a326271c439b6f9e174f3b48ed132bbff71c00592c7dbd072ccdc334445bde2                             18 seconds ago       Running             grafana-sc-dashboard               0                   8ce98cef767e8       monitoring-grafana-68567f8765-gd2tt
784fc6cc477ae       quay.io/prometheus-operator/prometheus-config-reloader@sha256:78aec597d87aa2b4ba947ab9190538dae93a58a67b8e930aefea1086534b02ef   33 seconds ago       Exited              init-config-reloader               0                   79c513d9bb120       prometheus-monitoring-kube-prometheus-prometheus-0
ccecf6587ac2c       quay.io/prometheus-operator/prometheus-config-reloader@sha256:78aec597d87aa2b4ba947ab9190538dae93a58a67b8e930aefea1086534b02ef   36 seconds ago       Exited              init-config-reloader               0                   0aca40a0e69c4       alertmanager-monitoring-kube-prometheus-alertmanager-0
702eaa0c1436d       curlimages/curl@sha256:8addc281f0ea517409209f76832b6ddc2cabc3264feb1ebbec2a2521ffad24e4                                          45 seconds ago       Exited              download-dashboards                0                   8ce98cef767e8       monitoring-grafana-68567f8765-gd2tt
bc166b131fce8       quay.io/prometheus-operator/prometheus-operator@sha256:b6a89b8ec08f4cca759b2d579e8545f97ffb897973fcd68148c153f2e936c8b3          54 seconds ago       Running             kube-prometheus-stack              0                   de43b8f5ee651       monitoring-kube-prometheus-operator-559cccc57f-jn9rv
645c2449dd112       registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:db384bf43222b066c378e77027a675d4cd9911107adba46c2922b3a55e10d6fb    About a minute ago   Running             kube-state-metrics                 0                   5053c2339ac69       monitoring-kube-state-metrics-59bd6f86f9-g6t4l
20bcab94d6271       quay.io/prometheus/node-exporter@sha256:d00a542e409ee618a4edc67da14dd48c5da66726bbd5537ab2af9c1dfc442c8a                         About a minute ago   Running             node-exporter                      0                   d04ad9dd62f1d       monitoring-prometheus-node-exporter-n7j42
a6af9e0f656fb       saotech/k8s_backend@sha256:83571513fdbf392475a96d6ab49dbfdb1827ec1be27059b118c3626efe52afd8                                      23 minutes ago       Running             backend-container                  0                   bb1e2eba718cf       backend-app-8c7cf5bc6-fplkb
0af466554d195       saotech/k8s_frontend@sha256:96064c0872c47d7e77a16593218aed80f64fee5acfcc1dd9c8bf83389a07c51f                                     2 hours ago          Running             react-container                    0                   ccbe8ed350805       react-frontend-5d487db865-hwl75
0be7169f941d5       ghcr.io/dexidp/dex@sha256:bc7cfce7c17f52864e2bb2a4dc1d2f86a41e3019f6d42e81d92a301fad0c8a1d                                       5 hours ago          Running             dex                                1                   16e7d5e3fbc16       argocd-dex-server-7967947859-9nlp6
515555c1e8c04       redis@sha256:ddd16a9b1575a774c7e62956be8daa1de5b32cfb5c25b7a216aefed8e0919f9b                                                    5 hours ago          Running             redis                              0                   c348033ec1438       argocd-redis-566477488c-w4cvp
08360315dcc62       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                                  5 hours ago          Running             argocd-server                      0                   a590cc736d67d       argocd-server-86d9c99cff-8drwr
651e66804bbbf       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                                  5 hours ago          Running             argocd-repo-server                 0                   ef9e029efe5db       argocd-repo-server-5c8756cd4f-h8zx9
331fb1eea630a       ghcr.io/dexidp/dex@sha256:bc7cfce7c17f52864e2bb2a4dc1d2f86a41e3019f6d42e81d92a301fad0c8a1d                                       5 hours ago          Exited              dex                                0                   16e7d5e3fbc16       argocd-dex-server-7967947859-9nlp6
6a248ec8242e5       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                                  5 hours ago          Running             argocd-application-controller      0                   c5bde69decd49       argocd-application-controller-0
aa6e0fdba30de       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                                  5 hours ago          Exited              secret-init                        0                   c348033ec1438       argocd-redis-566477488c-w4cvp
327cd510765a0       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                                  5 hours ago          Exited              copyutil                           0                   ef9e029efe5db       argocd-repo-server-5c8756cd4f-h8zx9
9b91cc18364c5       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                                  5 hours ago          Running             argocd-notifications-controller    0                   7bbffce665570       argocd-notifications-controller-7776574f76-tbrfk
245341e76ba3b       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                                  5 hours ago          Exited              copyutil                           0                   16e7d5e3fbc16       argocd-dex-server-7967947859-9nlp6
9204c497522c8       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                                  5 hours ago          Running             argocd-applicationset-controller   0                   31b74e762faaa       argocd-applicationset-controller-77cb676b6c-99lbn
a910d393a0c1f       6e38f40d628db                                                                                                                    5 hours ago          Running             storage-provisioner                11                  b4df896d912fe       storage-provisioner
8eaf17ffcb859       07655ddf2eebe                                                                                                                    5 hours ago          Running             kubernetes-dashboard               11                  a0f8d5d77c3aa       kubernetes-dashboard-8694d4445c-tw474
b7af75b5e9c83       5aa0bf4798fa2                                                                                                                    5 hours ago          Running             controller                         1                   9b092dfe44e39       ingress-nginx-controller-56b4bdd5c7-85vbh
bbe672297ab7c       115053965e86b                                                                                                                    5 hours ago          Running             dashboard-metrics-scraper          5                   5667277b93c59       dashboard-metrics-scraper-7fd5cb4ddc-6fxx6
7269a73e2291b       07655ddf2eebe                                                                                                                    5 hours ago          Exited              kubernetes-dashboard               10                  a0f8d5d77c3aa       kubernetes-dashboard-8694d4445c-tw474
e7d05ebbe299f       6e38f40d628db                                                                                                                    5 hours ago          Exited              storage-provisioner                10                  b4df896d912fe       storage-provisioner
5bd01ff0e8d58       ead0a4a53df89                                                                                                                    5 hours ago          Running             coredns                            5                   8fc808cacf71e       coredns-5dd5756b68-kczsx
9785b857c6174       bfc896cf80fba                                                                                                                    5 hours ago          Running             kube-proxy                         5                   2fd09553906f2       kube-proxy-t56xn
172a3b1087314       10baa1ca17068                                                                                                                    5 hours ago          Running             kube-controller-manager            5                   e36fa0d566495       kube-controller-manager-minikube
7c8a6f44be118       73deb9a3f7025                                                                                                                    5 hours ago          Running             etcd                               5                   7e860ae9a5a6b       etcd-minikube
286e3fefacdb7       5374347291230                                                                                                                    5 hours ago          Running             kube-apiserver                     5                   c7f4cb2d0ce18       kube-apiserver-minikube
6511951e15778       6d1b4fd1b182d                                                                                                                    5 hours ago          Running             kube-scheduler                     5                   aef3acab8501c       kube-scheduler-minikube
38fee0baa4ec0       5aa0bf4798fa2                                                                                                                    22 hours ago         Exited              controller                         0                   8aa1f5380e6c6       ingress-nginx-controller-56b4bdd5c7-85vbh
c803a3e7bd6f6       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80       29 hours ago         Exited              patch                              0                   dc1afa8d2c934       ingress-nginx-admission-patch-ftp7j
5c2eb40332a3d       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80       29 hours ago         Exited              create                             0                   d564db0d38460       ingress-nginx-admission-create-d58tz
7e322d8ff9748       115053965e86b                                                                                                                    29 hours ago         Exited              dashboard-metrics-scraper          4                   0cacd28bc9130       dashboard-metrics-scraper-7fd5cb4ddc-6fxx6
da2c7c1aa2d26       ead0a4a53df89                                                                                                                    29 hours ago         Exited              coredns                            4                   0bb4b548ac786       coredns-5dd5756b68-kczsx
6838dd5e06adc       bfc896cf80fba                                                                                                                    29 hours ago         Exited              kube-proxy                         4                   ad726e855a8bc       kube-proxy-t56xn
5dbcf03e87c1a       6d1b4fd1b182d                                                                                                                    29 hours ago         Exited              kube-scheduler                     4                   c655f9af2ffe0       kube-scheduler-minikube
1404c722267d7       10baa1ca17068                                                                                                                    29 hours ago         Exited              kube-controller-manager            4                   96ba22ff689d0       kube-controller-manager-minikube
3e62202256d31       73deb9a3f7025                                                                                                                    29 hours ago         Exited              etcd                               4                   c292efab8d4bf       etcd-minikube
d478b56162552       5374347291230                                                                                                                    29 hours ago         Exited              kube-apiserver                     4                   d6066c9ec5298       kube-apiserver-minikube

* 
* ==> controller_ingress [38fee0baa4ec] <==
* W0619 20:21:04.207493       7 client_config.go:618] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0619 20:21:04.208289       7 main.go:205] "Creating API client" host="https://10.96.0.1:443"
I0619 20:21:04.218134       7 main.go:249] "Running in Kubernetes cluster" major="1" minor="28" git="v1.28.3" state="clean" commit="a8a1abc25cad87333840cd7d54be2efaf31a3177" platform="linux/amd64"
I0619 20:21:04.373706       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0619 20:21:04.405908       7 ssl.go:536] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0619 20:21:04.423026       7 nginx.go:260] "Starting NGINX Ingress controller"
I0619 20:21:04.432337       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"e17f1960-9a5b-44ca-b84d-93f86b3244c9", APIVersion:"v1", ResourceVersion:"40819", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0619 20:21:04.436247       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"87271a31-768e-4971-bb80-5c67921d7fde", APIVersion:"v1", ResourceVersion:"40820", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0619 20:21:04.436379       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"f4d7a0be-520b-48a6-b0f5-75287abda00b", APIVersion:"v1", ResourceVersion:"40821", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0619 20:21:05.531903       7 store.go:440] "Found valid IngressClass" ingress="default/backend-ingress" ingressclass="nginx"
I0619 20:21:05.533058       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"backend-ingress", UID:"51d94bfc-adc7-463a-bb83-b98f7ff63657", APIVersion:"networking.k8s.io/v1", ResourceVersion:"58984", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0619 20:21:05.533511       7 store.go:440] "Found valid IngressClass" ingress="default/frontend-ingress" ingressclass="nginx"
I0619 20:21:05.534139       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"frontend-ingress", UID:"f173c3c2-947f-43b5-9341-9b6161dd1f17", APIVersion:"networking.k8s.io/v1", ResourceVersion:"58983", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0619 20:21:05.626180       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0619 20:21:05.626118       7 nginx.go:303] "Starting NGINX process"
I0619 20:21:05.628612       7 nginx.go:323] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0619 20:21:05.629472       7 controller.go:190] "Configuration changes detected, backend reload required"
I0619 20:21:05.633928       7 status.go:84] "New leader elected" identity="ingress-nginx-controller-7c6974c4d8-6qqhn"
I0619 20:21:05.764957       7 controller.go:210] "Backend successfully reloaded"
I0619 20:21:05.765078       7 controller.go:221] "Initial sync, sleeping for 1 second"
I0619 20:21:05.765509       7 event.go:298] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-56b4bdd5c7-85vbh", UID:"493b4bed-eea7-48d0-9355-156f9f3b33f8", APIVersion:"v1", ResourceVersion:"58988", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I0619 20:21:36.799005       7 status.go:84] "New leader elected" identity="ingress-nginx-controller-56b4bdd5c7-85vbh"
I0619 20:21:36.798991       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0619 20:21:36.811705       7 status.go:304] "updating Ingress status" namespace="default" ingress="frontend-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I0619 20:21:36.811863       7 status.go:304] "updating Ingress status" namespace="default" ingress="backend-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I0619 20:21:36.821186       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"frontend-ingress", UID:"f173c3c2-947f-43b5-9341-9b6161dd1f17", APIVersion:"networking.k8s.io/v1", ResourceVersion:"59039", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0619 20:21:36.822263       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"backend-ingress", UID:"51d94bfc-adc7-463a-bb83-b98f7ff63657", APIVersion:"networking.k8s.io/v1", ResourceVersion:"59040", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0619 23:45:11.845541       7 sigterm.go:36] "Received SIGTERM, shutting down"
I0619 23:45:11.845732       7 nginx.go:379] "Shutting down controller queues"
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.9.4
  Build:         846d251814a09d8a5d8d28e2e604bfc7749bcb49
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.21.6

-------------------------------------------------------------------------------


* 
* ==> controller_ingress [b7af75b5e9c8] <==
* W0620 13:12:22.692964       7 client_config.go:618] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0620 13:12:22.693500       7 main.go:205] "Creating API client" host="https://10.96.0.1:443"
W0620 13:12:33.870652       7 main.go:246] Initial connection to the Kubernetes API server was retried 1 times.
I0620 13:12:33.870718       7 main.go:249] "Running in Kubernetes cluster" major="1" minor="28" git="v1.28.3" state="clean" commit="a8a1abc25cad87333840cd7d54be2efaf31a3177" platform="linux/amd64"
I0620 13:12:34.268065       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0620 13:12:34.302092       7 ssl.go:536] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0620 13:12:34.317361       7 nginx.go:260] "Starting NGINX Ingress controller"
I0620 13:12:34.330316       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"e17f1960-9a5b-44ca-b84d-93f86b3244c9", APIVersion:"v1", ResourceVersion:"40819", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0620 13:12:34.335186       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"87271a31-768e-4971-bb80-5c67921d7fde", APIVersion:"v1", ResourceVersion:"40820", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0620 13:12:34.335230       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"f4d7a0be-520b-48a6-b0f5-75287abda00b", APIVersion:"v1", ResourceVersion:"40821", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0620 13:12:35.422400       7 store.go:440] "Found valid IngressClass" ingress="default/backend-ingress" ingressclass="nginx"
I0620 13:12:35.422878       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"backend-ingress", UID:"51d94bfc-adc7-463a-bb83-b98f7ff63657", APIVersion:"networking.k8s.io/v1", ResourceVersion:"59040", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0620 13:12:35.423414       7 store.go:440] "Found valid IngressClass" ingress="default/frontend-ingress" ingressclass="nginx"
I0620 13:12:35.423742       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"frontend-ingress", UID:"f173c3c2-947f-43b5-9341-9b6161dd1f17", APIVersion:"networking.k8s.io/v1", ResourceVersion:"59039", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0620 13:12:35.520638       7 nginx.go:303] "Starting NGINX process"
I0620 13:12:35.520872       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0620 13:12:35.521434       7 nginx.go:323] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0620 13:12:35.523040       7 controller.go:190] "Configuration changes detected, backend reload required"
I0620 13:12:35.534678       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0620 13:12:35.534840       7 status.go:84] "New leader elected" identity="ingress-nginx-controller-56b4bdd5c7-85vbh"
I0620 13:12:35.540354       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-56b4bdd5c7-85vbh" node="minikube"
I0620 13:12:35.547095       7 status.go:304] "updating Ingress status" namespace="default" ingress="frontend-ingress" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I0620 13:12:35.547131       7 status.go:304] "updating Ingress status" namespace="default" ingress="backend-ingress" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I0620 13:12:35.554200       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"backend-ingress", UID:"51d94bfc-adc7-463a-bb83-b98f7ff63657", APIVersion:"networking.k8s.io/v1", ResourceVersion:"59826", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0620 13:12:35.554797       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"frontend-ingress", UID:"f173c3c2-947f-43b5-9341-9b6161dd1f17", APIVersion:"networking.k8s.io/v1", ResourceVersion:"59827", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0620 13:12:35.634649       7 controller.go:210] "Backend successfully reloaded"
I0620 13:12:35.634864       7 controller.go:221] "Initial sync, sleeping for 1 second"
I0620 13:12:35.634975       7 event.go:298] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-56b4bdd5c7-85vbh", UID:"493b4bed-eea7-48d0-9355-156f9f3b33f8", APIVersion:"v1", ResourceVersion:"59728", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I0620 13:13:35.548028       7 status.go:304] "updating Ingress status" namespace="default" ingress="backend-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I0620 13:13:35.548426       7 status.go:304] "updating Ingress status" namespace="default" ingress="frontend-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I0620 13:13:35.555550       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"backend-ingress", UID:"51d94bfc-adc7-463a-bb83-b98f7ff63657", APIVersion:"networking.k8s.io/v1", ResourceVersion:"59898", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0620 13:13:35.555969       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"frontend-ingress", UID:"f173c3c2-947f-43b5-9341-9b6161dd1f17", APIVersion:"networking.k8s.io/v1", ResourceVersion:"59899", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0620 15:00:21.783040       7 admission.go:149] processed ingress via admission controller {testedIngressLength:2 testedIngressTime:0.144s renderingIngressLength:2 renderingIngressTime:0.001s admissionTime:22.0kBs testedConfigurationSize:0.145}
I0620 15:00:21.783211       7 main.go:107] "successfully validated configuration, accepting" ingress="default/frontend-ingress"
I0620 15:00:21.797525       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"frontend-ingress", UID:"f173c3c2-947f-43b5-9341-9b6161dd1f17", APIVersion:"networking.k8s.io/v1", ResourceVersion:"63284", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0620 15:00:31.905850       7 admission.go:149] processed ingress via admission controller {testedIngressLength:2 testedIngressTime:0.109s renderingIngressLength:2 renderingIngressTime:0.001s admissionTime:22.0kBs testedConfigurationSize:0.11}
I0620 15:00:31.905900       7 main.go:107] "successfully validated configuration, accepting" ingress="default/backend-ingress"
I0620 15:00:31.913198       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"backend-ingress", UID:"51d94bfc-adc7-463a-bb83-b98f7ff63657", APIVersion:"networking.k8s.io/v1", ResourceVersion:"63314", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0620 16:21:09.095315       7 admission.go:149] processed ingress via admission controller {testedIngressLength:2 testedIngressTime:1.609s renderingIngressLength:2 renderingIngressTime:0.016s admissionTime:25.7kBs testedConfigurationSize:1.626}
I0620 16:21:09.095678       7 main.go:107] "successfully validated configuration, accepting" ingress="default/backend-ingress"
I0620 16:21:09.105906       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"backend-ingress", UID:"51d94bfc-adc7-463a-bb83-b98f7ff63657", APIVersion:"networking.k8s.io/v1", ResourceVersion:"67667", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0620 16:21:11.102177       7 controller.go:190] "Configuration changes detected, backend reload required"
I0620 16:21:11.219139       7 controller.go:210] "Backend successfully reloaded"
I0620 16:21:11.219715       7 event.go:298] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-56b4bdd5c7-85vbh", UID:"493b4bed-eea7-48d0-9355-156f9f3b33f8", APIVersion:"v1", ResourceVersion:"59728", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.9.4
  Build:         846d251814a09d8a5d8d28e2e604bfc7749bcb49
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.21.6

-------------------------------------------------------------------------------


* 
* ==> coredns [5bd01ff0e8d5] <==
* [INFO] 10.244.0.254:47973 - 32929 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000192898s
[INFO] 10.244.0.254:39099 - 60415 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000234997s
[INFO] 10.244.0.254:35891 - 46091 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 125 0.022050959s
[INFO] 10.244.0.254:45120 - 23871 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.023376444s
[INFO] 10.244.1.2:41438 - 4280 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.0001923s
[INFO] 10.244.1.2:44732 - 51548 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.0001719s
[INFO] 10.244.1.2:33803 - 57026 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000317376s
[INFO] 10.244.1.2:43347 - 37444 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000392171s
[INFO] 10.244.1.2:50665 - 44012 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000249897s
[INFO] 10.244.1.2:46241 - 37763 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000345396s
[INFO] 10.244.1.2:38157 - 49664 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000260901s
[INFO] 10.244.1.2:52489 - 32741 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.0003946s
[INFO] 10.244.0.254:46732 - 34603 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.0002013s
[INFO] 10.244.0.254:58399 - 48418 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000344401s
[INFO] 10.244.0.254:46563 - 63706 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000246501s
[INFO] 10.244.0.254:47476 - 15266 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000356501s
[INFO] 10.244.0.254:48452 - 34365 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.0001885s
[INFO] 10.244.0.254:34727 - 62765 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000290801s
[INFO] 10.244.0.254:48939 - 56332 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 125 0.034504669s
[INFO] 10.244.0.254:51138 - 5920 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.041826684s
[INFO] 10.244.1.2:41664 - 9253 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.001304216s
[INFO] 10.244.1.2:55980 - 60473 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.001955522s
[INFO] 10.244.1.2:33727 - 52306 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000286202s
[INFO] 10.244.1.2:55829 - 14590 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000273502s
[INFO] 10.244.1.2:36231 - 25423 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000450726s
[INFO] 10.244.1.2:56458 - 3245 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000444427s
[INFO] 10.244.0.254:56901 - 17030 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000212965s
[INFO] 10.244.0.254:51104 - 11344 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000296551s
[INFO] 10.244.0.254:60315 - 16586 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000184969s
[INFO] 10.244.0.254:59230 - 57491 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000252158s
[INFO] 10.244.0.254:36416 - 4070 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000223463s
[INFO] 10.244.0.254:54908 - 29612 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.00030015s
[INFO] 10.244.0.254:37960 - 11126 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.009755497s
[INFO] 10.244.0.254:54466 - 57269 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 125 0.098071981s
[INFO] 10.244.1.2:41807 - 16328 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000487697s
[INFO] 10.244.1.2:36618 - 35218 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000811195s
[INFO] 10.244.1.2:48532 - 9620 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000274502s
[INFO] 10.244.1.2:50936 - 27804 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000348202s
[INFO] 10.244.0.254:48630 - 17643 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000255201s
[INFO] 10.244.0.254:55964 - 25357 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000285102s
[INFO] 10.244.0.254:37648 - 43420 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.0001451s
[INFO] 10.244.0.254:59476 - 30902 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000144001s
[INFO] 10.244.0.254:42398 - 38582 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000141001s
[INFO] 10.244.0.254:50964 - 13488 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000142501s
[INFO] 10.244.0.254:56825 - 35607 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 1.062841543s
[INFO] 10.244.0.254:54199 - 54391 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 125 1.069182378s
[INFO] 10.244.1.2:43175 - 44176 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000449901s
[INFO] 10.244.1.2:36559 - 61405 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000502101s
[INFO] 10.244.1.2:42279 - 24321 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000842497s
[INFO] 10.244.1.2:50071 - 42281 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.001176596s
[INFO] 10.244.0.254:41220 - 63250 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000267399s
[INFO] 10.244.0.254:36302 - 26868 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000323899s
[INFO] 10.244.0.254:57543 - 27375 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000312599s
[INFO] 10.244.0.254:34298 - 60887 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000211899s
[INFO] 10.244.0.254:59655 - 22886 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000149399s
[INFO] 10.244.0.254:43267 - 26311 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000204799s
[INFO] 10.244.0.254:54926 - 18798 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 125 0.015456848s
[INFO] 10.244.0.254:58316 - 58289 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.019020536s
[INFO] 10.244.1.2:43761 - 56928 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000228701s
[INFO] 10.244.1.2:48655 - 64402 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000410301s

* 
* ==> coredns [da2c7c1aa2d2] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:60917 - 34414 "HINFO IN 7363103934492017414.7181476431573564374. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.064328353s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_04_15T16_57_09_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 15 Apr 2025 11:27:05 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 20 Jun 2025 18:04:36 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 20 Jun 2025 18:04:08 +0000   Tue, 15 Apr 2025 11:27:03 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 20 Jun 2025 18:04:08 +0000   Tue, 15 Apr 2025 11:27:03 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 20 Jun 2025 18:04:08 +0000   Tue, 15 Apr 2025 11:27:03 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 20 Jun 2025 18:04:08 +0000   Tue, 15 Apr 2025 11:27:19 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  263112772Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8065360Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  263112772Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8065360Ki
  pods:               110
System Info:
  Machine ID:                 b4a4ef3ada97480d80da58f8abc55cc3
  System UUID:                b4a4ef3ada97480d80da58f8abc55cc3
  Boot ID:                    9e2bfaf1-c1a4-46af-91b5-cf7e0d28ec3a
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (25 in total)
  Namespace                   Name                                                      CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                      ------------  ----------  ---------------  -------------  ---
  argocd                      argocd-application-controller-0                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h50m
  argocd                      argocd-applicationset-controller-77cb676b6c-99lbn         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h50m
  argocd                      argocd-dex-server-7967947859-9nlp6                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h50m
  argocd                      argocd-notifications-controller-7776574f76-tbrfk          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h50m
  argocd                      argocd-redis-566477488c-w4cvp                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h50m
  argocd                      argocd-repo-server-5c8756cd4f-h8zx9                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h50m
  argocd                      argocd-server-86d9c99cff-8drwr                            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h50m
  default                     backend-app-8c7cf5bc6-fplkb                               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23m
  default                     react-frontend-5d487db865-hwl75                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         99m
  ingress-nginx               ingress-nginx-controller-56b4bdd5c7-85vbh                 100m (0%!)(MISSING)     0 (0%!)(MISSING)      90Mi (1%!)(MISSING)        0 (0%!)(MISSING)         21h
  kube-system                 coredns-5dd5756b68-kczsx                                  100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     66d
  kube-system                 etcd-minikube                                             100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         66d
  kube-system                 kube-apiserver-minikube                                   250m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         66d
  kube-system                 kube-controller-manager-minikube                          200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         66d
  kube-system                 kube-proxy-t56xn                                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         66d
  kube-system                 kube-scheduler-minikube                                   100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         66d
  kube-system                 storage-provisioner                                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         66d
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-6fxx6                0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         66d
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-tw474                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         66d
  monitoring                  alertmanager-monitoring-kube-prometheus-alertmanager-0    0 (0%!)(MISSING)        0 (0%!)(MISSING)      200Mi (2%!)(MISSING)       0 (0%!)(MISSING)         53s
  monitoring                  monitoring-grafana-68567f8765-gd2tt                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         95s
  monitoring                  monitoring-kube-prometheus-operator-559cccc57f-jn9rv      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         95s
  monitoring                  monitoring-kube-state-metrics-59bd6f86f9-g6t4l            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         95s
  monitoring                  monitoring-prometheus-node-exporter-n7j42                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         95s
  monitoring                  prometheus-monitoring-kube-prometheus-prometheus-0        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         51s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (5%!)(MISSING)   0 (0%!)(MISSING)
  memory             460Mi (5%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [  +0.002024] WSL (1 - init(docker-desktop-data)) ERROR: UtilCreateProcessAndWait:688: /bin/mount failed with status 0xff00
[  +0.001384] WSL (1 - init(docker-desktop-data)) ERROR: ConfigMountFsTab:2594: Processing fstab with mount -a failed.
[  +0.003372] WSL (1 - init(docker-desktop-data)) ERROR: ConfigApplyWindowsLibPath:2542: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.007778] WSL (3 - init(docker-desktop-data)) ERROR: UtilCreateProcessAndWait:666: /bin/mount failed with 2
[  +0.001810] WSL (1 - init(docker-desktop-data)) ERROR: UtilCreateProcessAndWait:688: /bin/mount failed with status 0xff00
[  +0.002698] WSL (4 - init(docker-desktop-data)) ERROR: UtilCreateProcessAndWait:666: /bin/mount failed with 2
[  +0.002066] WSL (1 - init(docker-desktop-data)) ERROR: UtilCreateProcessAndWait:688: /bin/mount failed with status 0xff00
[  +0.003600] WSL (5 - init(docker-desktop-data)) ERROR: UtilCreateProcessAndWait:666: /bin/mount failed with 2
[  +0.002034] WSL (1 - init(docker-desktop-data)) ERROR: UtilCreateProcessAndWait:688: /bin/mount failed with status 0xff00
[  +0.004524] WSL (1 - init(docker-desktop-data)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.118208] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.003273] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001160] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000999] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000939] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001046] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.003025] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000955] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001200] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001218] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.402721] netlink: 'init': attribute type 4 has an invalid length.
[  +0.814105] kmem.limit_in_bytes is deprecated and will be removed. Please report your usecase to linux-mm@kvack.org if you depend on this functionality.
[  +0.912565] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001504] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.286683] WSL (208) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +0.491873] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000066] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000819] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000688] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000762] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000701] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000591] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000678] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000708] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000595] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000555] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000836] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001253] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001063] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000923] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000855] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000647] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000773] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000778] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000760] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000751] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000715] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000692] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000723] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000770] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000881] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000627] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000683] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.630293] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.012545] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[ +33.855423] WSL (208) ERROR: CheckConnection: getaddrinfo() failed: -5
[Jun20 15:53] WSL (208) ERROR: CheckConnection: getaddrinfo() failed: -5
[Jun20 17:10] WSL (208) ERROR: CheckConnection: getaddrinfo() failed: -5
[Jun20 17:21] WSL (208) ERROR: CheckConnection: getaddrinfo() failed: -5
[Jun20 17:41] WSL (208) ERROR: CheckConnection: getaddrinfo() failed: -5

* 
* ==> etcd [3e62202256d3] <==
* {"level":"info","ts":"2025-06-19T19:43:25.129762Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":70007,"local-member-snapshot-index":60006,"local-member-snapshot-count":10000}
{"level":"info","ts":"2025-06-19T19:43:25.158069Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":70007}
{"level":"info","ts":"2025-06-19T19:43:25.158661Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":65007}
{"level":"info","ts":"2025-06-19T19:43:51.660699Z","caller":"fileutil/purge.go:85","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000003-0000000000004e22.snap"}
{"level":"info","ts":"2025-06-19T19:43:51.969124Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":56457}
{"level":"info","ts":"2025-06-19T19:43:51.971085Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":56457,"took":"1.624402ms","hash":806748277}
{"level":"info","ts":"2025-06-19T19:43:51.971194Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":806748277,"revision":56457,"compact-revision":56163}
{"level":"info","ts":"2025-06-19T19:47:22.253293Z","caller":"traceutil/trace.go:171","msg":"trace[159706605] transaction","detail":"{read_only:false; response_revision:57030; number_of_response:1; }","duration":"104.331137ms","start":"2025-06-19T19:47:22.148891Z","end":"2025-06-19T19:47:22.253222Z","steps":["trace[159706605] 'process raft request'  (duration: 103.708343ms)"],"step_count":1}
{"level":"info","ts":"2025-06-19T19:48:51.9899Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":56741}
{"level":"info","ts":"2025-06-19T19:48:51.991215Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":56741,"took":"922.013Âµs","hash":2978392958}
{"level":"info","ts":"2025-06-19T19:48:51.99125Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2978392958,"revision":56741,"compact-revision":56457}
{"level":"info","ts":"2025-06-19T19:50:48.744601Z","caller":"traceutil/trace.go:171","msg":"trace[591875989] transaction","detail":"{read_only:false; response_revision:57222; number_of_response:1; }","duration":"106.333393ms","start":"2025-06-19T19:50:48.638233Z","end":"2025-06-19T19:50:48.744567Z","steps":["trace[591875989] 'process raft request'  (duration: 105.926293ms)"],"step_count":1}
{"level":"info","ts":"2025-06-19T19:50:48.744884Z","caller":"traceutil/trace.go:171","msg":"trace[1790818640] linearizableReadLoop","detail":"{readStateIndex:70608; appliedIndex:70606; }","duration":"105.660593ms","start":"2025-06-19T19:50:48.639017Z","end":"2025-06-19T19:50:48.744677Z","steps":["trace[1790818640] 'read index received'  (duration: 99.384306ms)","trace[1790818640] 'applied index is now lower than readState.Index'  (duration: 6.272087ms)"],"step_count":2}
{"level":"warn","ts":"2025-06-19T19:50:48.746388Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"106.133293ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-06-19T19:50:48.746877Z","caller":"traceutil/trace.go:171","msg":"trace[995205519] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:57222; }","duration":"107.736889ms","start":"2025-06-19T19:50:48.638953Z","end":"2025-06-19T19:50:48.74669Z","steps":["trace[995205519] 'agreement among raft nodes before linearized reading'  (duration: 105.980393ms)"],"step_count":1}
{"level":"info","ts":"2025-06-19T19:52:11.340318Z","caller":"traceutil/trace.go:171","msg":"trace[572499846] transaction","detail":"{read_only:false; response_revision:57298; number_of_response:1; }","duration":"100.051271ms","start":"2025-06-19T19:52:11.240216Z","end":"2025-06-19T19:52:11.340268Z","steps":["trace[572499846] 'process raft request'  (duration: 99.682677ms)"],"step_count":1}
{"level":"info","ts":"2025-06-19T19:53:52.000941Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":57115}
{"level":"info","ts":"2025-06-19T19:53:52.003057Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":57115,"took":"1.647094ms","hash":46167907}
{"level":"info","ts":"2025-06-19T19:53:52.003147Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":46167907,"revision":57115,"compact-revision":56741}
{"level":"info","ts":"2025-06-19T19:58:52.021764Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":57393}
{"level":"info","ts":"2025-06-19T19:58:52.023404Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":57393,"took":"1.172904ms","hash":1867816013}
{"level":"info","ts":"2025-06-19T19:58:52.023465Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1867816013,"revision":57393,"compact-revision":57115}
{"level":"info","ts":"2025-06-19T20:03:52.03343Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":57671}
{"level":"info","ts":"2025-06-19T20:03:52.034885Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":57671,"took":"999.504Âµs","hash":2871484559}
{"level":"info","ts":"2025-06-19T20:03:52.034921Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2871484559,"revision":57671,"compact-revision":57393}
{"level":"warn","ts":"2025-06-19T20:06:22.228976Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"195.94798ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2025-06-19T20:06:22.229346Z","caller":"traceutil/trace.go:171","msg":"trace[1805514592] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:58091; }","duration":"196.190782ms","start":"2025-06-19T20:06:22.03296Z","end":"2025-06-19T20:06:22.229151Z","steps":["trace[1805514592] 'range keys from in-memory index tree'  (duration: 195.660777ms)"],"step_count":1}
{"level":"info","ts":"2025-06-19T20:06:22.533478Z","caller":"traceutil/trace.go:171","msg":"trace[153837956] transaction","detail":"{read_only:false; response_revision:58092; number_of_response:1; }","duration":"202.872939ms","start":"2025-06-19T20:06:22.330557Z","end":"2025-06-19T20:06:22.53343Z","steps":["trace[153837956] 'process raft request'  (duration: 202.600937ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-19T20:06:50.038186Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.147865ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2025-06-19T20:06:50.038438Z","caller":"traceutil/trace.go:171","msg":"trace[1205149983] range","detail":"{range_begin:/registry/secrets/; range_end:/registry/secrets0; response_count:0; response_revision:58147; }","duration":"109.507863ms","start":"2025-06-19T20:06:49.92879Z","end":"2025-06-19T20:06:50.038298Z","steps":["trace[1205149983] 'count revisions from in-memory index tree'  (duration: 108.911165ms)"],"step_count":1}
{"level":"info","ts":"2025-06-19T20:08:52.043595Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":57950}
{"level":"info","ts":"2025-06-19T20:08:52.045618Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":57950,"took":"1.527811ms","hash":3981122434}
{"level":"info","ts":"2025-06-19T20:08:52.0457Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3981122434,"revision":57950,"compact-revision":57671}
{"level":"info","ts":"2025-06-19T20:10:07.639621Z","caller":"traceutil/trace.go:171","msg":"trace[1994400513] transaction","detail":"{read_only:false; response_revision:58340; number_of_response:1; }","duration":"107.063312ms","start":"2025-06-19T20:10:07.532495Z","end":"2025-06-19T20:10:07.639558Z","steps":["trace[1994400513] 'process raft request'  (duration: 106.511912ms)"],"step_count":1}
{"level":"info","ts":"2025-06-19T20:13:52.07089Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":58269}
{"level":"info","ts":"2025-06-19T20:13:52.072803Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":58269,"took":"1.435599ms","hash":3326686323}
{"level":"info","ts":"2025-06-19T20:13:52.072888Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3326686323,"revision":58269,"compact-revision":57950}
{"level":"info","ts":"2025-06-19T20:18:52.099053Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":58548}
{"level":"info","ts":"2025-06-19T20:18:52.102362Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":58548,"took":"2.782003ms","hash":2890957099}
{"level":"info","ts":"2025-06-19T20:18:52.102513Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2890957099,"revision":58548,"compact-revision":58269}
{"level":"warn","ts":"2025-06-19T20:21:12.425187Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"197.660709ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-06-19T20:21:12.425394Z","caller":"traceutil/trace.go:171","msg":"trace[747318305] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:59010; }","duration":"198.047911ms","start":"2025-06-19T20:21:12.227308Z","end":"2025-06-19T20:21:12.425356Z","steps":["trace[747318305] 'range keys from in-memory index tree'  (duration: 197.572208ms)"],"step_count":1}
{"level":"info","ts":"2025-06-19T20:23:52.120302Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":58832}
{"level":"info","ts":"2025-06-19T20:23:52.122099Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":58832,"took":"1.330122ms","hash":953726650}
{"level":"info","ts":"2025-06-19T20:23:52.122136Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":953726650,"revision":58832,"compact-revision":58548}
{"level":"info","ts":"2025-06-19T20:24:40.827839Z","caller":"traceutil/trace.go:171","msg":"trace[930860373] transaction","detail":"{read_only:false; response_revision:59214; number_of_response:1; }","duration":"103.081938ms","start":"2025-06-19T20:24:40.724725Z","end":"2025-06-19T20:24:40.827807Z","steps":["trace[930860373] 'process raft request'  (duration: 102.840138ms)"],"step_count":1}
{"level":"info","ts":"2025-06-19T20:28:52.126937Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":59167}
{"level":"info","ts":"2025-06-19T20:28:52.128638Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":59167,"took":"1.313556ms","hash":2086999569}
{"level":"info","ts":"2025-06-19T20:28:52.12868Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2086999569,"revision":59167,"compact-revision":58832}
{"level":"info","ts":"2025-06-19T20:29:10.022291Z","caller":"traceutil/trace.go:171","msg":"trace[260458590] transaction","detail":"{read_only:false; response_revision:59464; number_of_response:1; }","duration":"101.248552ms","start":"2025-06-19T20:29:09.920985Z","end":"2025-06-19T20:29:10.022233Z","steps":["trace[260458590] 'process raft request'  (duration: 100.866389ms)"],"step_count":1}
{"level":"info","ts":"2025-06-19T23:45:11.842621Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-06-19T23:45:11.843961Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-06-19T23:45:11.845344Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-06-19T23:45:11.845944Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-06-19T23:45:12.037845Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-06-19T23:45:12.03791Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-06-19T23:45:12.039677Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-06-19T23:45:12.240191Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-19T23:45:12.240718Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-19T23:45:12.240809Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [7c8a6f44be11] <==
* {"level":"info","ts":"2025-06-20T17:49:36.212075Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4160972235,"revision":68441,"compact-revision":68083}
{"level":"info","ts":"2025-06-20T17:51:26.916473Z","caller":"traceutil/trace.go:171","msg":"trace[196033493] linearizableReadLoop","detail":"{readStateIndex:84601; appliedIndex:84600; }","duration":"194.974339ms","start":"2025-06-20T17:51:26.721443Z","end":"2025-06-20T17:51:26.916417Z","steps":["trace[196033493] 'read index received'  (duration: 194.494343ms)","trace[196033493] 'applied index is now lower than readState.Index'  (duration: 477.296Âµs)"],"step_count":2}
{"level":"info","ts":"2025-06-20T17:51:26.917204Z","caller":"traceutil/trace.go:171","msg":"trace[205437608] transaction","detail":"{read_only:false; response_revision:68901; number_of_response:1; }","duration":"196.264031ms","start":"2025-06-20T17:51:26.720887Z","end":"2025-06-20T17:51:26.917151Z","steps":["trace[205437608] 'process raft request'  (duration: 195.081038ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-20T17:51:26.917277Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"195.829033ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-06-20T17:51:26.917553Z","caller":"traceutil/trace.go:171","msg":"trace[1203487363] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:68901; }","duration":"196.090032ms","start":"2025-06-20T17:51:26.721375Z","end":"2025-06-20T17:51:26.917465Z","steps":["trace[1203487363] 'agreement among raft nodes before linearized reading'  (duration: 195.580235ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T17:54:36.211189Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":68797}
{"level":"info","ts":"2025-06-20T17:54:36.22702Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":68797,"took":"15.154115ms","hash":1928073584}
{"level":"info","ts":"2025-06-20T17:54:36.22714Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1928073584,"revision":68797,"compact-revision":68441}
{"level":"info","ts":"2025-06-20T17:59:36.232098Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":69082}
{"level":"info","ts":"2025-06-20T17:59:36.236137Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":69082,"took":"3.203305ms","hash":213958907}
{"level":"info","ts":"2025-06-20T17:59:36.236207Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":213958907,"revision":69082,"compact-revision":68797}
{"level":"info","ts":"2025-06-20T18:00:25.312159Z","caller":"traceutil/trace.go:171","msg":"trace[1882141036] transaction","detail":"{read_only:false; response_revision:69410; number_of_response:1; }","duration":"102.125218ms","start":"2025-06-20T18:00:25.210001Z","end":"2025-06-20T18:00:25.312127Z","steps":["trace[1882141036] 'process raft request'  (duration: 101.864414ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:01:40.618191Z","caller":"traceutil/trace.go:171","msg":"trace[1236731931] transaction","detail":"{read_only:false; response_revision:69482; number_of_response:1; }","duration":"102.048859ms","start":"2025-06-20T18:01:40.516109Z","end":"2025-06-20T18:01:40.618158Z","steps":["trace[1236731931] 'process raft request'  (duration: 101.538353ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:02.416102Z","caller":"traceutil/trace.go:171","msg":"trace[983372907] transaction","detail":"{read_only:false; response_revision:69703; number_of_response:1; }","duration":"203.089699ms","start":"2025-06-20T18:03:02.212991Z","end":"2025-06-20T18:03:02.416081Z","steps":["trace[983372907] 'process raft request'  (duration: 116.539305ms)","trace[983372907] 'compare'  (duration: 86.345086ms)"],"step_count":2}
{"level":"info","ts":"2025-06-20T18:03:02.416214Z","caller":"traceutil/trace.go:171","msg":"trace[1041400221] transaction","detail":"{read_only:false; response_revision:69704; number_of_response:1; }","duration":"202.61478ms","start":"2025-06-20T18:03:02.213588Z","end":"2025-06-20T18:03:02.416203Z","steps":["trace[1041400221] 'process raft request'  (duration: 202.403872ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:02.416296Z","caller":"traceutil/trace.go:171","msg":"trace[16724887] transaction","detail":"{read_only:false; response_revision:69705; number_of_response:1; }","duration":"201.725444ms","start":"2025-06-20T18:03:02.214554Z","end":"2025-06-20T18:03:02.416279Z","steps":["trace[16724887] 'process raft request'  (duration: 201.496535ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:02.416506Z","caller":"traceutil/trace.go:171","msg":"trace[1850258575] transaction","detail":"{read_only:false; response_revision:69706; number_of_response:1; }","duration":"196.738843ms","start":"2025-06-20T18:03:02.219755Z","end":"2025-06-20T18:03:02.416494Z","steps":["trace[1850258575] 'process raft request'  (duration: 196.464332ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:02.416586Z","caller":"traceutil/trace.go:171","msg":"trace[1052278374] transaction","detail":"{read_only:false; response_revision:69707; number_of_response:1; }","duration":"194.043734ms","start":"2025-06-20T18:03:02.222527Z","end":"2025-06-20T18:03:02.416571Z","steps":["trace[1052278374] 'process raft request'  (duration: 193.845426ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:02.416572Z","caller":"traceutil/trace.go:171","msg":"trace[385875603] transaction","detail":"{read_only:false; response_revision:69708; number_of_response:1; }","duration":"193.043195ms","start":"2025-06-20T18:03:02.223516Z","end":"2025-06-20T18:03:02.416559Z","steps":["trace[385875603] 'process raft request'  (duration: 192.969192ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:02.817723Z","caller":"traceutil/trace.go:171","msg":"trace[790195732] transaction","detail":"{read_only:false; response_revision:69736; number_of_response:1; }","duration":"102.553641ms","start":"2025-06-20T18:03:02.715118Z","end":"2025-06-20T18:03:02.817672Z","steps":["trace[790195732] 'process raft request'  (duration: 102.219628ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:02.817776Z","caller":"traceutil/trace.go:171","msg":"trace[1939917540] transaction","detail":"{read_only:false; response_revision:69735; number_of_response:1; }","duration":"104.116404ms","start":"2025-06-20T18:03:02.71356Z","end":"2025-06-20T18:03:02.817677Z","steps":["trace[1939917540] 'process raft request'  (duration: 18.672054ms)","trace[1939917540] 'marshal mvccpb.KeyValue' {req_type:put; key:/registry/events/monitoring/monitoring-grafana-68567f8765-gd2tt.184ad24130a3c8d9; req_size:747; } (duration: 84.842526ms)"],"step_count":2}
{"level":"info","ts":"2025-06-20T18:03:03.124535Z","caller":"traceutil/trace.go:171","msg":"trace[1459054522] transaction","detail":"{read_only:false; response_revision:69740; number_of_response:1; }","duration":"105.25865ms","start":"2025-06-20T18:03:03.019206Z","end":"2025-06-20T18:03:03.124465Z","steps":["trace[1459054522] 'process raft request'  (duration: 94.962934ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:03.12482Z","caller":"traceutil/trace.go:171","msg":"trace[453901583] transaction","detail":"{read_only:false; response_revision:69741; number_of_response:1; }","duration":"100.664065ms","start":"2025-06-20T18:03:03.024112Z","end":"2025-06-20T18:03:03.124776Z","steps":["trace[453901583] 'process raft request'  (duration: 100.092742ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:04.613406Z","caller":"traceutil/trace.go:171","msg":"trace[1822821067] transaction","detail":"{read_only:false; response_revision:69748; number_of_response:1; }","duration":"181.918983ms","start":"2025-06-20T18:03:04.431439Z","end":"2025-06-20T18:03:04.613358Z","steps":["trace[1822821067] 'process raft request'  (duration: 181.655792ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:12.83163Z","caller":"traceutil/trace.go:171","msg":"trace[13946748] transaction","detail":"{read_only:false; response_revision:69776; number_of_response:1; }","duration":"106.31619ms","start":"2025-06-20T18:03:12.725261Z","end":"2025-06-20T18:03:12.831577Z","steps":["trace[13946748] 'process raft request'  (duration: 105.977994ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:12.831877Z","caller":"traceutil/trace.go:171","msg":"trace[708001649] transaction","detail":"{read_only:false; response_revision:69777; number_of_response:1; }","duration":"105.570099ms","start":"2025-06-20T18:03:12.726258Z","end":"2025-06-20T18:03:12.831828Z","steps":["trace[708001649] 'process raft request'  (duration: 105.099504ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:12.832183Z","caller":"traceutil/trace.go:171","msg":"trace[837841566] transaction","detail":"{read_only:false; response_revision:69780; number_of_response:1; }","duration":"104.64971ms","start":"2025-06-20T18:03:12.727508Z","end":"2025-06-20T18:03:12.832157Z","steps":["trace[837841566] 'process raft request'  (duration: 104.300314ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:12.832379Z","caller":"traceutil/trace.go:171","msg":"trace[548856899] transaction","detail":"{read_only:false; response_revision:69779; number_of_response:1; }","duration":"104.774409ms","start":"2025-06-20T18:03:12.727357Z","end":"2025-06-20T18:03:12.832132Z","steps":["trace[548856899] 'process raft request'  (duration: 104.333114ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:12.832448Z","caller":"traceutil/trace.go:171","msg":"trace[37058198] transaction","detail":"{read_only:false; response_revision:69781; number_of_response:1; }","duration":"104.63671ms","start":"2025-06-20T18:03:12.727632Z","end":"2025-06-20T18:03:12.832269Z","steps":["trace[37058198] 'process raft request'  (duration: 104.374213ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:12.832507Z","caller":"traceutil/trace.go:171","msg":"trace[1302484172] transaction","detail":"{read_only:false; response_revision:69782; number_of_response:1; }","duration":"104.751109ms","start":"2025-06-20T18:03:12.727723Z","end":"2025-06-20T18:03:12.832474Z","steps":["trace[1302484172] 'process raft request'  (duration: 104.396413ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:12.83215Z","caller":"traceutil/trace.go:171","msg":"trace[564857371] transaction","detail":"{read_only:false; response_revision:69778; number_of_response:1; }","duration":"105.190903ms","start":"2025-06-20T18:03:12.726916Z","end":"2025-06-20T18:03:12.832107Z","steps":["trace[564857371] 'process raft request'  (duration: 104.539111ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:12.832626Z","caller":"traceutil/trace.go:171","msg":"trace[2098690936] transaction","detail":"{read_only:false; response_revision:69783; number_of_response:1; }","duration":"104.773108ms","start":"2025-06-20T18:03:12.72781Z","end":"2025-06-20T18:03:12.832583Z","steps":["trace[2098690936] 'process raft request'  (duration: 104.404113ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:12.832081Z","caller":"traceutil/trace.go:171","msg":"trace[369588105] transaction","detail":"{read_only:false; response_revision:69775; number_of_response:1; }","duration":"109.624849ms","start":"2025-06-20T18:03:12.72241Z","end":"2025-06-20T18:03:12.832035Z","steps":["trace[369588105] 'process raft request'  (duration: 100.823258ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:12.832948Z","caller":"traceutil/trace.go:171","msg":"trace[791789762] transaction","detail":"{read_only:false; response_revision:69784; number_of_response:1; }","duration":"105.089704ms","start":"2025-06-20T18:03:12.727827Z","end":"2025-06-20T18:03:12.832916Z","steps":["trace[791789762] 'process raft request'  (duration: 104.566011ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:12.833281Z","caller":"traceutil/trace.go:171","msg":"trace[1932356919] transaction","detail":"{read_only:false; response_revision:69785; number_of_response:1; }","duration":"105.4155ms","start":"2025-06-20T18:03:12.727824Z","end":"2025-06-20T18:03:12.833239Z","steps":["trace[1932356919] 'process raft request'  (duration: 104.671509ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:12.83362Z","caller":"traceutil/trace.go:171","msg":"trace[660759100] transaction","detail":"{read_only:false; response_revision:69786; number_of_response:1; }","duration":"105.390701ms","start":"2025-06-20T18:03:12.728122Z","end":"2025-06-20T18:03:12.833513Z","steps":["trace[660759100] 'process raft request'  (duration: 105.004906ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:12.833673Z","caller":"traceutil/trace.go:171","msg":"trace[802610023] transaction","detail":"{read_only:false; response_revision:69787; number_of_response:1; }","duration":"105.4088ms","start":"2025-06-20T18:03:12.728231Z","end":"2025-06-20T18:03:12.833641Z","steps":["trace[802610023] 'process raft request'  (duration: 105.021305ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:12.834094Z","caller":"traceutil/trace.go:171","msg":"trace[257264884] transaction","detail":"{read_only:false; response_revision:69788; number_of_response:1; }","duration":"105.977593ms","start":"2025-06-20T18:03:12.728075Z","end":"2025-06-20T18:03:12.834053Z","steps":["trace[257264884] 'process raft request'  (duration: 105.284602ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:12.834232Z","caller":"traceutil/trace.go:171","msg":"trace[975413533] transaction","detail":"{read_only:false; response_revision:69790; number_of_response:1; }","duration":"101.416649ms","start":"2025-06-20T18:03:12.732778Z","end":"2025-06-20T18:03:12.834195Z","steps":["trace[975413533] 'process raft request'  (duration: 101.142253ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:12.834126Z","caller":"traceutil/trace.go:171","msg":"trace[2020558200] transaction","detail":"{read_only:false; response_revision:69789; number_of_response:1; }","duration":"103.704522ms","start":"2025-06-20T18:03:12.730384Z","end":"2025-06-20T18:03:12.834088Z","steps":["trace[2020558200] 'process raft request'  (duration: 103.273827ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:12.834522Z","caller":"traceutil/trace.go:171","msg":"trace[248133756] transaction","detail":"{read_only:false; response_revision:69791; number_of_response:1; }","duration":"100.011367ms","start":"2025-06-20T18:03:12.73447Z","end":"2025-06-20T18:03:12.834482Z","steps":["trace[248133756] 'process raft request'  (duration: 99.572972ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:44.914383Z","caller":"traceutil/trace.go:171","msg":"trace[1479313147] transaction","detail":"{read_only:false; response_revision:69928; number_of_response:1; }","duration":"200.594877ms","start":"2025-06-20T18:03:44.713736Z","end":"2025-06-20T18:03:44.914331Z","steps":["trace[1479313147] 'process raft request'  (duration: 200.132077ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:44.914697Z","caller":"traceutil/trace.go:171","msg":"trace[534570341] linearizableReadLoop","detail":"{readStateIndex:85795; appliedIndex:85795; }","duration":"200.459976ms","start":"2025-06-20T18:03:44.714168Z","end":"2025-06-20T18:03:44.914628Z","steps":["trace[534570341] 'read index received'  (duration: 200.334376ms)","trace[534570341] 'applied index is now lower than readState.Index'  (duration: 120Âµs)"],"step_count":2}
{"level":"warn","ts":"2025-06-20T18:03:44.915177Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"201.005277ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/statefulsets/monitoring/alertmanager-monitoring-kube-prometheus-alertmanager\" ","response":"range_response_count:1 size:11428"}
{"level":"info","ts":"2025-06-20T18:03:44.915488Z","caller":"traceutil/trace.go:171","msg":"trace[737501473] range","detail":"{range_begin:/registry/statefulsets/monitoring/alertmanager-monitoring-kube-prometheus-alertmanager; range_end:; response_count:1; response_revision:69928; }","duration":"201.341677ms","start":"2025-06-20T18:03:44.7141Z","end":"2025-06-20T18:03:44.915442Z","steps":["trace[737501473] 'agreement among raft nodes before linearized reading'  (duration: 200.799677ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:44.929018Z","caller":"traceutil/trace.go:171","msg":"trace[1510034873] transaction","detail":"{read_only:false; response_revision:69929; number_of_response:1; }","duration":"114.489301ms","start":"2025-06-20T18:03:44.814483Z","end":"2025-06-20T18:03:44.928972Z","steps":["trace[1510034873] 'process raft request'  (duration: 113.950601ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:45.036298Z","caller":"traceutil/trace.go:171","msg":"trace[765717800] linearizableReadLoop","detail":"{readStateIndex:85798; appliedIndex:85795; }","duration":"121.217207ms","start":"2025-06-20T18:03:44.915051Z","end":"2025-06-20T18:03:45.036268Z","steps":["trace[765717800] 'read index received'  (duration: 13.363212ms)","trace[765717800] 'applied index is now lower than readState.Index'  (duration: 107.852595ms)"],"step_count":2}
{"level":"warn","ts":"2025-06-20T18:03:45.036677Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"223.274197ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/monitoring/monitoring-kube-prometheus-alertmanager\" ","response":"range_response_count:1 size:1209"}
{"level":"info","ts":"2025-06-20T18:03:45.036747Z","caller":"traceutil/trace.go:171","msg":"trace[386987125] range","detail":"{range_begin:/registry/serviceaccounts/monitoring/monitoring-kube-prometheus-alertmanager; range_end:; response_count:1; response_revision:69931; }","duration":"223.358097ms","start":"2025-06-20T18:03:44.81337Z","end":"2025-06-20T18:03:45.036728Z","steps":["trace[386987125] 'agreement among raft nodes before linearized reading'  (duration: 223.109497ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:03:45.037383Z","caller":"traceutil/trace.go:171","msg":"trace[210781921] transaction","detail":"{read_only:false; response_revision:69930; number_of_response:1; }","duration":"123.177609ms","start":"2025-06-20T18:03:44.91418Z","end":"2025-06-20T18:03:45.037358Z","steps":["trace[210781921] 'process raft request'  (duration: 106.782094ms)","trace[210781921] 'compare'  (duration: 14.985414ms)"],"step_count":2}
{"level":"info","ts":"2025-06-20T18:03:45.037776Z","caller":"traceutil/trace.go:171","msg":"trace[537335602] transaction","detail":"{read_only:false; response_revision:69931; number_of_response:1; }","duration":"123.435909ms","start":"2025-06-20T18:03:44.914321Z","end":"2025-06-20T18:03:45.037757Z","steps":["trace[537335602] 'process raft request'  (duration: 121.826408ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-20T18:03:45.038052Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"123.316508ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-06-20T18:03:45.038104Z","caller":"traceutil/trace.go:171","msg":"trace[1459198937] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:69931; }","duration":"123.478109ms","start":"2025-06-20T18:03:44.914611Z","end":"2025-06-20T18:03:45.03809Z","steps":["trace[1459198937] 'agreement among raft nodes before linearized reading'  (duration: 123.380309ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-20T18:03:45.038347Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"124.19811ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-06-20T18:03:45.038394Z","caller":"traceutil/trace.go:171","msg":"trace[122348548] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:69931; }","duration":"124.25171ms","start":"2025-06-20T18:03:44.91413Z","end":"2025-06-20T18:03:45.038382Z","steps":["trace[122348548] 'agreement among raft nodes before linearized reading'  (duration: 124.17261ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-20T18:03:45.038887Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"224.086698ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/\" range_end:\"/registry/deployments0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2025-06-20T18:03:45.038945Z","caller":"traceutil/trace.go:171","msg":"trace[579494534] range","detail":"{range_begin:/registry/deployments/; range_end:/registry/deployments0; response_count:0; response_revision:69931; }","duration":"224.165398ms","start":"2025-06-20T18:03:44.814765Z","end":"2025-06-20T18:03:45.038931Z","steps":["trace[579494534] 'agreement among raft nodes before linearized reading'  (duration: 224.022098ms)"],"step_count":1}
{"level":"info","ts":"2025-06-20T18:04:36.249502Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":69364}
{"level":"info","ts":"2025-06-20T18:04:36.252816Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":69364,"took":"2.632723ms","hash":3727719009}
{"level":"info","ts":"2025-06-20T18:04:36.252887Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3727719009,"revision":69364,"compact-revision":69082}

* 
* ==> kernel <==
*  18:04:37 up  2:37,  0 users,  load average: 1.48, 0.80, 0.61
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [286e3fefacdb] <==
* Trace[340251439]: ["Call mutating webhook" configuration:monitoring-kube-prometheus-admission,webhook:prometheusrulemutate.monitoring.coreos.com,resource:monitoring.coreos.com/v1, Resource=prometheusrules,subresource:,operation:CREATE,UID:65fc66ea-05d5-46d2-9329-ba4cb8458540 10193ms (18:03:02.719)]
Trace[340251439]: [10.198252277s] [10.198252277s] END
I0620 18:03:12.914229       1 trace.go:236] Trace[1734321199]: "Create" accept:application/json,audit-id:392eff81-6c47-472b-aaf5-1a1ade03cca0,client:192.168.49.1,protocol:HTTP/2.0,resource:prometheusrules,scope:resource,url:/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules,user-agent:Helm/3.14.0,verb:POST (20-Jun-2025 18:03:02.714) (total time: 10199ms):
Trace[1734321199]: ["Call mutating webhook" configuration:monitoring-kube-prometheus-admission,webhook:prometheusrulemutate.monitoring.coreos.com,resource:monitoring.coreos.com/v1, Resource=prometheusrules,subresource:,operation:CREATE,UID:74368b87-6402-48cc-964c-a6c782d62f21 10193ms (18:03:02.720)]
Trace[1734321199]: [10.199970725s] [10.199970725s] END
I0620 18:03:12.914353       1 trace.go:236] Trace[1599766856]: "Create" accept:application/json,audit-id:383c70b5-4e54-49b6-9000-0bebdfd540d5,client:192.168.49.1,protocol:HTTP/2.0,resource:prometheusrules,scope:resource,url:/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules,user-agent:Helm/3.14.0,verb:POST (20-Jun-2025 18:03:02.626) (total time: 10287ms):
Trace[1599766856]: ---"limitedReadBody succeeded" len:3800 90ms (18:03:02.716)
Trace[1599766856]: ["Call mutating webhook" configuration:monitoring-kube-prometheus-admission,webhook:prometheusrulemutate.monitoring.coreos.com,resource:monitoring.coreos.com/v1, Resource=prometheusrules,subresource:,operation:CREATE,UID:a7707441-425d-474f-8745-b3f94f0e8589 10196ms (18:03:02.717)]
Trace[1599766856]: ---"Writing http response done" 76ms (18:03:12.914)
Trace[1599766856]: [10.287973272s] [10.287973272s] END
I0620 18:03:12.914476       1 trace.go:236] Trace[1639011133]: "Create" accept:application/json,audit-id:6320d1ae-a44a-46ea-a766-f03847558c18,client:192.168.49.1,protocol:HTTP/2.0,resource:prometheusrules,scope:resource,url:/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules,user-agent:Helm/3.14.0,verb:POST (20-Jun-2025 18:03:02.629) (total time: 10284ms):
Trace[1639011133]: ---"limitedReadBody succeeded" len:5394 88ms (18:03:02.718)
Trace[1639011133]: ["Call mutating webhook" configuration:monitoring-kube-prometheus-admission,webhook:prometheusrulemutate.monitoring.coreos.com,resource:monitoring.coreos.com/v1, Resource=prometheusrules,subresource:,operation:CREATE,UID:139af394-2408-4694-af1d-04a25d25f9bd 10194ms (18:03:02.720)]
Trace[1639011133]: [10.284701633s] [10.284701633s] END
I0620 18:03:12.914650       1 trace.go:236] Trace[1245393466]: "Create" accept:application/json,audit-id:ad1ea5cd-b9ae-493d-a40b-21cdf64fb2af,client:192.168.49.1,protocol:HTTP/2.0,resource:prometheusrules,scope:resource,url:/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules,user-agent:Helm/3.14.0,verb:POST (20-Jun-2025 18:03:02.624) (total time: 10290ms):
Trace[1245393466]: ---"Conversion done" 88ms (18:03:02.713)
Trace[1245393466]: ["Call mutating webhook" configuration:monitoring-kube-prometheus-admission,webhook:prometheusrulemutate.monitoring.coreos.com,resource:monitoring.coreos.com/v1, Resource=prometheusrules,subresource:,operation:CREATE,UID:eae06022-7e86-40cc-a5fb-a40bca5bb5d7 10195ms (18:03:02.719)]
Trace[1245393466]: [10.290105642s] [10.290105642s] END
I0620 18:03:12.914690       1 trace.go:236] Trace[268601126]: "Create" accept:application/json,audit-id:42333189-673d-4012-b162-2318107ce1b2,client:192.168.49.1,protocol:HTTP/2.0,resource:prometheusrules,scope:resource,url:/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules,user-agent:Helm/3.14.0,verb:POST (20-Jun-2025 18:03:02.528) (total time: 10386ms):
Trace[268601126]: ---"limitedReadBody succeeded" len:17212 187ms (18:03:02.715)
Trace[268601126]: ["Call mutating webhook" configuration:monitoring-kube-prometheus-admission,webhook:prometheusrulemutate.monitoring.coreos.com,resource:monitoring.coreos.com/v1, Resource=prometheusrules,subresource:,operation:CREATE,UID:d8d1ea96-b07c-45be-afba-5b3504812dd0 10197ms (18:03:02.717)]
Trace[268601126]: [10.386058115s] [10.386058115s] END
I0620 18:03:12.914761       1 trace.go:236] Trace[874901962]: "Create" accept:application/json,audit-id:0146052d-808d-42db-a025-baf883a4fc23,client:192.168.49.1,protocol:HTTP/2.0,resource:prometheusrules,scope:resource,url:/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules,user-agent:Helm/3.14.0,verb:POST (20-Jun-2025 18:03:02.715) (total time: 10198ms):
Trace[874901962]: ["Call mutating webhook" configuration:monitoring-kube-prometheus-admission,webhook:prometheusrulemutate.monitoring.coreos.com,resource:monitoring.coreos.com/v1, Resource=prometheusrules,subresource:,operation:CREATE,UID:02ae0676-818f-410a-8589-86351bdf5ef9 10193ms (18:03:02.720)]
Trace[874901962]: [10.198884053s] [10.198884053s] END
I0620 18:03:12.914945       1 trace.go:236] Trace[1485016052]: "Create" accept:application/json,audit-id:d20343b3-7daa-4d8b-8aca-38a89140213f,client:192.168.49.1,protocol:HTTP/2.0,resource:prometheusrules,scope:resource,url:/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules,user-agent:Helm/3.14.0,verb:POST (20-Jun-2025 18:03:02.531) (total time: 10383ms):
Trace[1485016052]: ---"limitedReadBody succeeded" len:5995 187ms (18:03:02.718)
Trace[1485016052]: ["Call mutating webhook" configuration:monitoring-kube-prometheus-admission,webhook:prometheusrulemutate.monitoring.coreos.com,resource:monitoring.coreos.com/v1, Resource=prometheusrules,subresource:,operation:CREATE,UID:0e95c6f0-8182-4a36-8295-c7241a4850d1 10195ms (18:03:02.719)]
Trace[1485016052]: [10.383708005s] [10.383708005s] END
I0620 18:03:12.914979       1 trace.go:236] Trace[1656070503]: "Create" accept:application/json,audit-id:a2d6df37-2e6b-4103-9ef3-60630846fa3f,client:192.168.49.1,protocol:HTTP/2.0,resource:prometheusrules,scope:resource,url:/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules,user-agent:Helm/3.14.0,verb:POST (20-Jun-2025 18:03:02.529) (total time: 10385ms):
Trace[1656070503]: ---"limitedReadBody succeeded" len:6245 188ms (18:03:02.717)
Trace[1656070503]: ["Call mutating webhook" configuration:monitoring-kube-prometheus-admission,webhook:prometheusrulemutate.monitoring.coreos.com,resource:monitoring.coreos.com/v1, Resource=prometheusrules,subresource:,operation:CREATE,UID:603b0872-d61b-4525-afc0-cb0fe35f6fb7 10196ms (18:03:02.718)]
Trace[1656070503]: [10.385153463s] [10.385153463s] END
I0620 18:03:12.915111       1 trace.go:236] Trace[614188859]: "Create" accept:application/json,audit-id:3b700673-4f4a-4124-b028-80d5f9afcf8e,client:192.168.49.1,protocol:HTTP/2.0,resource:prometheusrules,scope:resource,url:/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules,user-agent:Helm/3.14.0,verb:POST (20-Jun-2025 18:03:02.624) (total time: 10290ms):
Trace[614188859]: ---"limitedReadBody succeeded" len:16727 91ms (18:03:02.716)
Trace[614188859]: ["Call mutating webhook" configuration:monitoring-kube-prometheus-admission,webhook:prometheusrulemutate.monitoring.coreos.com,resource:monitoring.coreos.com/v1, Resource=prometheusrules,subresource:,operation:CREATE,UID:15c184ac-32f4-49f5-9cca-df34c155b36d 10197ms (18:03:02.717)]
Trace[614188859]: [10.290211622s] [10.290211622s] END
I0620 18:03:12.922081       1 controller.go:624] quota admission added evaluator for: servicemonitors.monitoring.coreos.com
I0620 18:03:44.918944       1 trace.go:236] Trace[14125892]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:23faa4b6-b37f-4212-91e0-a0c46b0d1929,client:192.168.49.2,protocol:HTTP/2.0,resource:statefulsets,scope:resource,url:/apis/apps/v1/namespaces/monitoring/statefulsets/alertmanager-monitoring-kube-prometheus-alertmanager/status,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:statefulset-controller,verb:PUT (20-Jun-2025 18:03:44.417) (total time: 501ms):
Trace[14125892]: ["GuaranteedUpdate etcd3" audit-id:23faa4b6-b37f-4212-91e0-a0c46b0d1929,key:/statefulsets/monitoring/alertmanager-monitoring-kube-prometheus-alertmanager,type:*apps.StatefulSet,resource:statefulsets.apps 500ms (18:03:44.418)]
Trace[14125892]: [501.066442ms] [501.066442ms] END
I0620 18:03:44.928067       1 trace.go:236] Trace[441777515]: "Patch" accept:application/json, */*,audit-id:c5bc9edc-5d4a-49f8-a847-75682995661a,client:10.244.1.14,protocol:HTTP/2.0,resource:alertmanagers,scope:resource,url:/apis/monitoring.coreos.com/v1/namespaces/monitoring/alertmanagers/monitoring-kube-prometheus-alertmanager/status,user-agent:PrometheusOperator/0.83.0,verb:APPLY (20-Jun-2025 18:03:44.418) (total time: 509ms):
Trace[441777515]: ["GuaranteedUpdate etcd3" audit-id:c5bc9edc-5d4a-49f8-a847-75682995661a,key:/monitoring.coreos.com/alertmanagers/monitoring/monitoring-kube-prometheus-alertmanager,type:*unstructured.Unstructured,resource:alertmanagers.monitoring.coreos.com 508ms (18:03:44.419)
Trace[441777515]:  ---"About to Encode" 200ms (18:03:44.620)
Trace[441777515]:  ---"Txn call completed" 294ms (18:03:44.916)]
Trace[441777515]: ---"About to check admission control" 195ms (18:03:44.615)
Trace[441777515]: ---"Object stored in database" 308ms (18:03:44.924)
Trace[441777515]: [509.247949ms] [509.247949ms] END
I0620 18:03:45.023176       1 trace.go:236] Trace[768688649]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:5e189b10-5142-433e-ae39-398317e02042,client:10.244.1.14,protocol:HTTP/2.0,resource:configmaps,scope:resource,url:/api/v1/namespaces/monitoring/configmaps,user-agent:PrometheusOperator/0.83.0,verb:POST (20-Jun-2025 18:03:44.417) (total time: 605ms):
Trace[768688649]: ["Create etcd3" audit-id:5e189b10-5142-433e-ae39-398317e02042,key:/configmaps/monitoring/prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0,type:*core.ConfigMap,resource:configmaps 581ms (18:03:44.441)
Trace[768688649]:  ---"Encode succeeded" len:153926 273ms (18:03:44.714)
Trace[768688649]:  ---"Txn call succeeded" 299ms (18:03:45.013)]
Trace[768688649]: [605.997335ms] [605.997335ms] END
I0620 18:03:45.116226       1 trace.go:236] Trace[1943249395]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:b906ad07-c66b-4a22-9d1c-91608ad7323e,client:192.168.49.2,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/monitoring/pods/alertmanager-monitoring-kube-prometheus-alertmanager-0/status,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PATCH (20-Jun-2025 18:03:44.422) (total time: 694ms):
Trace[1943249395]: ["GuaranteedUpdate etcd3" audit-id:b906ad07-c66b-4a22-9d1c-91608ad7323e,key:/pods/monitoring/alertmanager-monitoring-kube-prometheus-alertmanager-0,type:*core.Pod,resource:pods 690ms (18:03:44.425)
Trace[1943249395]:  ---"About to Encode" 393ms (18:03:44.821)
Trace[1943249395]:  ---"Txn call completed" 293ms (18:03:45.114)]
Trace[1943249395]: ---"About to check admission control" 391ms (18:03:44.818)
Trace[1943249395]: ---"Object stored in database" 296ms (18:03:45.115)
Trace[1943249395]: [694.001412ms] [694.001412ms] END

* 
* ==> kube-apiserver [d478b5616255] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0619 23:45:21.111816       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0619 23:45:21.166178       1 logging.go:59] [core] [Channel #124 SubChannel #125] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0619 23:45:21.173747       1 logging.go:59] [core] [Channel #73 SubChannel #74] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0619 23:45:21.173762       1 logging.go:59] [core] [Channel #142 SubChannel #143] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0619 23:45:21.198885       1 logging.go:59] [core] [Channel #61 SubChannel #62] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0619 23:45:21.235834       1 logging.go:59] [core] [Channel #10 SubChannel #11] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0619 23:45:21.244671       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [1404c722267d] <==
* I0619 19:44:41.870538       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-7c5788b4cf" duration="49.4Âµs"
I0619 19:44:41.880894       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-7c5788b4cf" duration="66.4Âµs"
I0619 19:44:43.624601       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-7c5788b4cf" duration="60.299Âµs"
I0619 19:45:23.075952       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-55fc6f849b" duration="8.819894ms"
I0619 19:45:23.076232       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-55fc6f849b" duration="53.102Âµs"
I0619 19:45:23.088513       1 event.go:307] "Event occurred" object="default/react-frontend" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set react-frontend-b448f9f66 to 0 from 1"
I0619 19:45:23.099574       1 event.go:307] "Event occurred" object="default/react-frontend-b448f9f66" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: react-frontend-b448f9f66-fc4xc"
I0619 19:45:23.106445       1 event.go:307] "Event occurred" object="default/react-frontend" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set react-frontend-55fc6f849b to 1 from 0"
I0619 19:45:23.117130       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-b448f9f66" duration="29.161472ms"
I0619 19:45:23.145096       1 event.go:307] "Event occurred" object="default/react-frontend-55fc6f849b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: react-frontend-55fc6f849b-8rfc8"
I0619 19:45:23.154943       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-55fc6f849b" duration="48.764324ms"
I0619 19:45:23.158015       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-b448f9f66" duration="40.782259ms"
I0619 19:45:23.158180       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-b448f9f66" duration="52.502Âµs"
I0619 19:45:23.165861       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-55fc6f849b" duration="10.856161ms"
I0619 19:45:23.184803       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-55fc6f849b" duration="18.857228ms"
I0619 19:45:23.184967       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-55fc6f849b" duration="54.002Âµs"
I0619 19:45:23.405334       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-b448f9f66" duration="102.303Âµs"
I0619 19:45:23.483287       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-b448f9f66" duration="44.701Âµs"
I0619 19:45:23.789883       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-b448f9f66" duration="55.502Âµs"
I0619 19:45:23.793577       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-b448f9f66" duration="86.803Âµs"
I0619 19:45:27.626576       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-55fc6f849b" duration="9.988006ms"
I0619 19:45:27.626694       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-55fc6f849b" duration="72.1Âµs"
I0619 19:45:27.634505       1 event.go:307] "Event occurred" object="default/react-frontend" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set react-frontend-7c5788b4cf to 0 from 1"
I0619 19:45:27.648249       1 event.go:307] "Event occurred" object="default/react-frontend-7c5788b4cf" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: react-frontend-7c5788b4cf-wpd9p"
I0619 19:45:27.660104       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-7c5788b4cf" duration="25.934415ms"
I0619 19:45:27.681550       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-7c5788b4cf" duration="21.373212ms"
I0619 19:45:27.681725       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-7c5788b4cf" duration="84.4Âµs"
I0619 19:45:27.681777       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-7c5788b4cf" duration="31Âµs"
I0619 19:45:28.044051       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-7c5788b4cf" duration="53.9Âµs"
I0619 19:45:28.638250       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-7c5788b4cf" duration="87.1Âµs"
I0619 19:45:28.655132       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-7c5788b4cf" duration="70.6Âµs"
I0619 19:45:28.666358       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-7c5788b4cf" duration="47.8Âµs"
I0619 19:45:28.672751       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/react-frontend-7c5788b4cf" duration="92.7Âµs"
I0619 20:06:46.392200       1 event.go:307] "Event occurred" object="default/backend-app" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set backend-app-759d8bcc67 to 1"
I0619 20:06:46.415925       1 event.go:307] "Event occurred" object="default/backend-app-759d8bcc67" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: backend-app-759d8bcc67-4r4kv"
I0619 20:06:46.428581       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-app-759d8bcc67" duration="36.651554ms"
I0619 20:06:46.437633       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-app-759d8bcc67" duration="9.001464ms"
I0619 20:06:46.437779       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-app-759d8bcc67" duration="39.8Âµs"
I0619 20:06:46.449776       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-app-759d8bcc67" duration="71.5Âµs"
I0619 20:06:48.149763       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-app-759d8bcc67" duration="8.625866ms"
I0619 20:06:48.149985       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-app-759d8bcc67" duration="102.4Âµs"
I0619 20:21:01.442549       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-56b4bdd5c7 to 1"
I0619 20:21:01.470973       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-7c6974c4d8 to 0 from 1"
I0619 20:21:01.478211       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller-56b4bdd5c7" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-56b4bdd5c7-85vbh"
I0619 20:21:01.525887       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56b4bdd5c7" duration="83.584017ms"
I0619 20:21:01.540127       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller-7c6974c4d8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-7c6974c4d8-6qqhn"
I0619 20:21:01.540701       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56b4bdd5c7" duration="14.736121ms"
I0619 20:21:01.540823       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56b4bdd5c7" duration="58.4Âµs"
I0619 20:21:01.625998       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="155.257418ms"
I0619 20:21:01.725721       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56b4bdd5c7" duration="103.9Âµs"
I0619 20:21:01.725897       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="99.792541ms"
I0619 20:21:01.725991       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="63.6Âµs"
I0619 20:21:01.730698       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint ingress-nginx/ingress-nginx-controller: Operation cannot be fulfilled on endpoints \"ingress-nginx-controller\": the object has been modified; please apply your changes to the latest version and try again"
I0619 20:21:03.245255       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56b4bdd5c7" duration="194.2Âµs"
I0619 20:21:03.264651       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56b4bdd5c7" duration="131Âµs"
I0619 20:21:05.144122       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56b4bdd5c7" duration="191.2Âµs"
I0619 20:21:23.649343       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56b4bdd5c7" duration="19.250836ms"
I0619 20:21:23.649692       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56b4bdd5c7" duration="163.401Âµs"
E0619 23:44:31.769811       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0619 23:44:32.335896       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"

* 
* ==> kube-controller-manager [172a3b108731] <==
* I0620 17:46:20.727295       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-create"
I0620 17:46:20.727847       1 event.go:307] "Event occurred" object="monitoring/monitoring-kube-prometheus-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0620 17:47:20.032459       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-create"
I0620 17:47:20.082712       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-create"
I0620 18:02:57.569355       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-create"
I0620 18:02:57.585404       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-create"
I0620 18:02:57.585492       1 event.go:307] "Event occurred" object="monitoring/monitoring-kube-prometheus-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: monitoring-kube-prometheus-admission-create-fcpjq"
I0620 18:02:57.619742       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-create"
I0620 18:02:57.619874       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-create"
I0620 18:02:57.636970       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-create"
I0620 18:02:58.937889       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-create"
I0620 18:03:00.273022       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-create"
I0620 18:03:01.018164       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-create"
I0620 18:03:01.298563       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-create"
I0620 18:03:01.321434       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-create"
I0620 18:03:01.339698       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-create"
I0620 18:03:01.339980       1 event.go:307] "Event occurred" object="monitoring/monitoring-kube-prometheus-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0620 18:03:01.479433       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-create"
I0620 18:03:02.174681       1 event.go:307] "Event occurred" object="monitoring/monitoring-kube-state-metrics" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set monitoring-kube-state-metrics-59bd6f86f9 to 1"
I0620 18:03:02.180179       1 event.go:307] "Event occurred" object="monitoring/monitoring-kube-prometheus-operator" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set monitoring-kube-prometheus-operator-559cccc57f to 1"
I0620 18:03:02.313876       1 event.go:307] "Event occurred" object="monitoring/monitoring-kube-state-metrics-59bd6f86f9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: monitoring-kube-state-metrics-59bd6f86f9-g6t4l"
I0620 18:03:02.419011       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-kube-state-metrics-59bd6f86f9" duration="244.417269ms"
I0620 18:03:02.419139       1 event.go:307] "Event occurred" object="monitoring/monitoring-kube-prometheus-operator-559cccc57f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: monitoring-kube-prometheus-operator-559cccc57f-jn9rv"
I0620 18:03:02.419219       1 event.go:307] "Event occurred" object="monitoring/monitoring-grafana" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set monitoring-grafana-68567f8765 to 1"
I0620 18:03:02.513917       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-kube-state-metrics-59bd6f86f9" duration="94.828329ms"
I0620 18:03:02.514360       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-kube-state-metrics-59bd6f86f9" duration="149.306Âµs"
I0620 18:03:02.514615       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-kube-prometheus-operator-559cccc57f" duration="336.449884ms"
I0620 18:03:02.514641       1 event.go:307] "Event occurred" object="monitoring/monitoring-grafana-68567f8765" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: monitoring-grafana-68567f8765-gd2tt"
I0620 18:03:02.516112       1 event.go:307] "Event occurred" object="monitoring/monitoring-prometheus-node-exporter" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: monitoring-prometheus-node-exporter-n7j42"
I0620 18:03:02.526016       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-kube-prometheus-operator-559cccc57f" duration="11.355958ms"
I0620 18:03:02.526269       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-kube-prometheus-operator-559cccc57f" duration="71.603Âµs"
I0620 18:03:02.526296       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-kube-state-metrics-59bd6f86f9" duration="59.903Âµs"
I0620 18:03:02.616180       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-grafana-68567f8765" duration="197.346268ms"
I0620 18:03:02.821464       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-kube-prometheus-operator-559cccc57f" duration="215.909Âµs"
I0620 18:03:03.217236       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-grafana-68567f8765" duration="596.775596ms"
I0620 18:03:03.217830       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-grafana-68567f8765" duration="475.519Âµs"
I0620 18:03:03.331528       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-grafana-68567f8765" duration="194.108Âµs"
I0620 18:03:14.570279       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-patch"
I0620 18:03:14.580331       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-patch"
I0620 18:03:14.580352       1 event.go:307] "Event occurred" object="monitoring/monitoring-kube-prometheus-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: monitoring-kube-prometheus-admission-patch-rchvz"
I0620 18:03:14.622347       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-patch"
I0620 18:03:14.625484       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-patch"
I0620 18:03:14.643089       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-patch"
I0620 18:03:15.664460       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-patch"
I0620 18:03:17.610442       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-patch"
I0620 18:03:17.722834       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-patch"
I0620 18:03:18.620312       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-patch"
I0620 18:03:18.631775       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-patch"
I0620 18:03:18.641584       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-patch"
I0620 18:03:18.641599       1 event.go:307] "Event occurred" object="monitoring/monitoring-kube-prometheus-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0620 18:03:18.771966       1 job_controller.go:562] "enqueueing job" key="monitoring/monitoring-kube-prometheus-admission-patch"
I0620 18:03:33.496193       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-kube-state-metrics-59bd6f86f9" duration="182.906Âµs"
I0620 18:03:42.926474       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-kube-prometheus-operator-559cccc57f" duration="103.6Âµs"
I0620 18:03:43.414990       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-kube-state-metrics-59bd6f86f9" duration="99.869488ms"
I0620 18:03:43.416393       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-kube-state-metrics-59bd6f86f9" duration="338.1Âµs"
I0620 18:03:43.619069       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-kube-prometheus-operator-559cccc57f" duration="198.142475ms"
I0620 18:03:43.629864       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-kube-prometheus-operator-559cccc57f" duration="216.601Âµs"
I0620 18:03:44.225913       1 event.go:307] "Event occurred" object="monitoring/alertmanager-monitoring-kube-prometheus-alertmanager" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod alertmanager-monitoring-kube-prometheus-alertmanager-0 in StatefulSet alertmanager-monitoring-kube-prometheus-alertmanager successful"
I0620 18:03:46.025264       1 event.go:307] "Event occurred" object="monitoring/prometheus-monitoring-kube-prometheus-prometheus" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod prometheus-monitoring-kube-prometheus-prometheus-0 in StatefulSet prometheus-monitoring-kube-prometheus-prometheus successful"
I0620 18:03:51.795396       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="monitoring/monitoring-grafana-68567f8765" duration="205.301Âµs"

* 
* ==> kube-proxy [6838dd5e06ad] <==
* I0619 12:36:07.660323       1 server_others.go:69] "Using iptables proxy"
I0619 12:36:07.858190       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0619 12:36:07.962980       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0619 12:36:07.966036       1 server_others.go:152] "Using iptables Proxier"
I0619 12:36:07.966112       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0619 12:36:07.966122       1 server_others.go:438] "Defaulting to no-op detect-local"
I0619 12:36:07.968287       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0619 12:36:07.968857       1 server.go:846] "Version info" version="v1.28.3"
I0619 12:36:07.968900       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0619 12:36:07.971382       1 config.go:97] "Starting endpoint slice config controller"
I0619 12:36:07.971559       1 config.go:188] "Starting service config controller"
I0619 12:36:07.971738       1 config.go:315] "Starting node config controller"
I0619 12:36:07.972736       1 shared_informer.go:311] Waiting for caches to sync for service config
I0619 12:36:07.972730       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0619 12:36:07.972760       1 shared_informer.go:311] Waiting for caches to sync for node config
I0619 12:36:08.073142       1 shared_informer.go:318] Caches are synced for node config
I0619 12:36:08.073208       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0619 12:36:08.073214       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-proxy [9785b857c617] <==
* I0620 13:12:19.988688       1 server_others.go:69] "Using iptables proxy"
I0620 13:12:20.196656       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0620 13:12:21.089409       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0620 13:12:21.093747       1 server_others.go:152] "Using iptables Proxier"
I0620 13:12:21.093843       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0620 13:12:21.093858       1 server_others.go:438] "Defaulting to no-op detect-local"
I0620 13:12:21.094338       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0620 13:12:21.095209       1 server.go:846] "Version info" version="v1.28.3"
I0620 13:12:21.095234       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0620 13:12:21.099187       1 config.go:97] "Starting endpoint slice config controller"
I0620 13:12:21.099422       1 config.go:188] "Starting service config controller"
I0620 13:12:21.099839       1 config.go:315] "Starting node config controller"
I0620 13:12:21.101288       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0620 13:12:21.184563       1 shared_informer.go:311] Waiting for caches to sync for node config
I0620 13:12:21.186989       1 shared_informer.go:311] Waiting for caches to sync for service config
I0620 13:12:21.202408       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0620 13:12:21.284945       1 shared_informer.go:318] Caches are synced for node config
I0620 13:12:21.288145       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [5dbcf03e87c1] <==
* I0619 12:36:00.789646       1 serving.go:348] Generated self-signed cert in-memory
I0619 12:36:02.756074       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0619 12:36:02.756126       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0619 12:36:02.763434       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0619 12:36:02.763465       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0619 12:36:02.763486       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0619 12:36:02.764093       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0619 12:36:02.764148       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0619 12:36:02.764512       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0619 12:36:02.764513       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0619 12:36:02.764527       1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0619 12:36:02.866894       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0619 12:36:02.867013       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0619 12:36:02.867055       1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
E0619 23:45:12.150863       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kube-scheduler [6511951e1577] <==
* I0620 13:12:10.303066       1 serving.go:348] Generated self-signed cert in-memory
I0620 13:12:13.913345       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0620 13:12:13.913405       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0620 13:12:13.923102       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0620 13:12:13.923171       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0620 13:12:13.923354       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0620 13:12:13.924108       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0620 13:12:13.924199       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0620 13:12:13.924322       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0620 13:12:13.924369       1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0620 13:12:13.924350       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0620 13:12:14.025017       1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0620 13:12:14.025192       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0620 13:12:14.025252       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file

* 
* ==> kubelet <==
* Jun 20 18:03:00 minikube kubelet[1713]: I0620 18:03:00.369857    1713 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/b90b0de9-c7c2-439a-bf74-b51bce640ba3-kube-api-access-zmw4x" (OuterVolumeSpecName: "kube-api-access-zmw4x") pod "b90b0de9-c7c2-439a-bf74-b51bce640ba3" (UID: "b90b0de9-c7c2-439a-bf74-b51bce640ba3"). InnerVolumeSpecName "kube-api-access-zmw4x". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jun 20 18:03:00 minikube kubelet[1713]: I0620 18:03:00.467428    1713 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-zmw4x\" (UniqueName: \"kubernetes.io/projected/b90b0de9-c7c2-439a-bf74-b51bce640ba3-kube-api-access-zmw4x\") on node \"minikube\" DevicePath \"\""
Jun 20 18:03:00 minikube kubelet[1713]: I0620 18:03:00.998586    1713 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="47506b3dfe75236e9a98fb527de332f8ef8d37fb6c339e0f9f6415173689615d"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.026907    1713 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="b90b0de9-c7c2-439a-bf74-b51bce640ba3" path="/var/lib/kubelet/pods/b90b0de9-c7c2-439a-bf74-b51bce640ba3/volumes"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.421466    1713 topology_manager.go:215] "Topology Admit Handler" podUID="d1f4274f-5cd2-40e7-a0e4-8881f18079b5" podNamespace="monitoring" podName="monitoring-kube-state-metrics-59bd6f86f9-g6t4l"
Jun 20 18:03:02 minikube kubelet[1713]: E0620 18:03:02.421875    1713 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="b90b0de9-c7c2-439a-bf74-b51bce640ba3" containerName="create"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.422087    1713 memory_manager.go:346] "RemoveStaleState removing state" podUID="b90b0de9-c7c2-439a-bf74-b51bce640ba3" containerName="create"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.427983    1713 topology_manager.go:215] "Topology Admit Handler" podUID="76047748-c0c0-4952-a0b5-cb8d9242814a" podNamespace="monitoring" podName="monitoring-kube-prometheus-operator-559cccc57f-jn9rv"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.627970    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-tz5xs\" (UniqueName: \"kubernetes.io/projected/d1f4274f-5cd2-40e7-a0e4-8881f18079b5-kube-api-access-tz5xs\") pod \"monitoring-kube-state-metrics-59bd6f86f9-g6t4l\" (UID: \"d1f4274f-5cd2-40e7-a0e4-8881f18079b5\") " pod="monitoring/monitoring-kube-state-metrics-59bd6f86f9-g6t4l"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.628453    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-rplhw\" (UniqueName: \"kubernetes.io/projected/76047748-c0c0-4952-a0b5-cb8d9242814a-kube-api-access-rplhw\") pod \"monitoring-kube-prometheus-operator-559cccc57f-jn9rv\" (UID: \"76047748-c0c0-4952-a0b5-cb8d9242814a\") " pod="monitoring/monitoring-kube-prometheus-operator-559cccc57f-jn9rv"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.628686    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tls-secret\" (UniqueName: \"kubernetes.io/secret/76047748-c0c0-4952-a0b5-cb8d9242814a-tls-secret\") pod \"monitoring-kube-prometheus-operator-559cccc57f-jn9rv\" (UID: \"76047748-c0c0-4952-a0b5-cb8d9242814a\") " pod="monitoring/monitoring-kube-prometheus-operator-559cccc57f-jn9rv"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.629136    1713 topology_manager.go:215] "Topology Admit Handler" podUID="bba30e15-4e92-42ba-a0e3-0a9592431ef7" podNamespace="monitoring" podName="monitoring-prometheus-node-exporter-n7j42"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.714821    1713 topology_manager.go:215] "Topology Admit Handler" podUID="14475042-632e-4ce4-b9a5-7746f3f3ff5e" podNamespace="monitoring" podName="monitoring-grafana-68567f8765-gd2tt"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.729149    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"sc-dashboard-volume\" (UniqueName: \"kubernetes.io/empty-dir/14475042-632e-4ce4-b9a5-7746f3f3ff5e-sc-dashboard-volume\") pod \"monitoring-grafana-68567f8765-gd2tt\" (UID: \"14475042-632e-4ce4-b9a5-7746f3f3ff5e\") " pod="monitoring/monitoring-grafana-68567f8765-gd2tt"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.729348    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"dashboards-default\" (UniqueName: \"kubernetes.io/configmap/14475042-632e-4ce4-b9a5-7746f3f3ff5e-dashboards-default\") pod \"monitoring-grafana-68567f8765-gd2tt\" (UID: \"14475042-632e-4ce4-b9a5-7746f3f3ff5e\") " pod="monitoring/monitoring-grafana-68567f8765-gd2tt"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.729391    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config\" (UniqueName: \"kubernetes.io/configmap/14475042-632e-4ce4-b9a5-7746f3f3ff5e-config\") pod \"monitoring-grafana-68567f8765-gd2tt\" (UID: \"14475042-632e-4ce4-b9a5-7746f3f3ff5e\") " pod="monitoring/monitoring-grafana-68567f8765-gd2tt"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.729424    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"sc-dashboard-provider\" (UniqueName: \"kubernetes.io/configmap/14475042-632e-4ce4-b9a5-7746f3f3ff5e-sc-dashboard-provider\") pod \"monitoring-grafana-68567f8765-gd2tt\" (UID: \"14475042-632e-4ce4-b9a5-7746f3f3ff5e\") " pod="monitoring/monitoring-grafana-68567f8765-gd2tt"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.729453    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6njxd\" (UniqueName: \"kubernetes.io/projected/14475042-632e-4ce4-b9a5-7746f3f3ff5e-kube-api-access-6njxd\") pod \"monitoring-grafana-68567f8765-gd2tt\" (UID: \"14475042-632e-4ce4-b9a5-7746f3f3ff5e\") " pod="monitoring/monitoring-grafana-68567f8765-gd2tt"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.729485    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"root\" (UniqueName: \"kubernetes.io/host-path/bba30e15-4e92-42ba-a0e3-0a9592431ef7-root\") pod \"monitoring-prometheus-node-exporter-n7j42\" (UID: \"bba30e15-4e92-42ba-a0e3-0a9592431ef7\") " pod="monitoring/monitoring-prometheus-node-exporter-n7j42"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.729518    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"storage\" (UniqueName: \"kubernetes.io/empty-dir/14475042-632e-4ce4-b9a5-7746f3f3ff5e-storage\") pod \"monitoring-grafana-68567f8765-gd2tt\" (UID: \"14475042-632e-4ce4-b9a5-7746f3f3ff5e\") " pod="monitoring/monitoring-grafana-68567f8765-gd2tt"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.729636    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"sys\" (UniqueName: \"kubernetes.io/host-path/bba30e15-4e92-42ba-a0e3-0a9592431ef7-sys\") pod \"monitoring-prometheus-node-exporter-n7j42\" (UID: \"bba30e15-4e92-42ba-a0e3-0a9592431ef7\") " pod="monitoring/monitoring-prometheus-node-exporter-n7j42"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.729669    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"sc-datasources-volume\" (UniqueName: \"kubernetes.io/empty-dir/14475042-632e-4ce4-b9a5-7746f3f3ff5e-sc-datasources-volume\") pod \"monitoring-grafana-68567f8765-gd2tt\" (UID: \"14475042-632e-4ce4-b9a5-7746f3f3ff5e\") " pod="monitoring/monitoring-grafana-68567f8765-gd2tt"
Jun 20 18:03:02 minikube kubelet[1713]: I0620 18:03:02.729722    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"proc\" (UniqueName: \"kubernetes.io/host-path/bba30e15-4e92-42ba-a0e3-0a9592431ef7-proc\") pod \"monitoring-prometheus-node-exporter-n7j42\" (UID: \"bba30e15-4e92-42ba-a0e3-0a9592431ef7\") " pod="monitoring/monitoring-prometheus-node-exporter-n7j42"
Jun 20 18:03:07 minikube kubelet[1713]: I0620 18:03:07.114819    1713 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="8ce98cef767e823cf043f7cac97806b72fa3fd6154e0f5fb6565dc010983cdd6"
Jun 20 18:03:07 minikube kubelet[1713]: I0620 18:03:07.130940    1713 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d04ad9dd62f1ddaea3da3b8b06ca93c085c9261aa86853a0813fd2c13a2664fa"
Jun 20 18:03:07 minikube kubelet[1713]: I0620 18:03:07.240760    1713 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5053c2339ac69a3a2ace0ea2855edcf16d264e733dca32b5c4222c15bddfc7c8"
Jun 20 18:03:07 minikube kubelet[1713]: I0620 18:03:07.340047    1713 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="de43b8f5ee6510dcebf6ebd238b93a5b73e54cab929d86ff1cbe7335d02f66ec"
Jun 20 18:03:14 minikube kubelet[1713]: I0620 18:03:14.623730    1713 topology_manager.go:215] "Topology Admit Handler" podUID="d57162f0-b718-463f-a69b-5809b4389d3a" podNamespace="monitoring" podName="monitoring-kube-prometheus-admission-patch-rchvz"
Jun 20 18:03:14 minikube kubelet[1713]: I0620 18:03:14.732366    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-tvmxf\" (UniqueName: \"kubernetes.io/projected/d57162f0-b718-463f-a69b-5809b4389d3a-kube-api-access-tvmxf\") pod \"monitoring-kube-prometheus-admission-patch-rchvz\" (UID: \"d57162f0-b718-463f-a69b-5809b4389d3a\") " pod="monitoring/monitoring-kube-prometheus-admission-patch-rchvz"
Jun 20 18:03:17 minikube kubelet[1713]: I0620 18:03:17.709024    1713 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="307b19ca96ca642e59358ca11817ba826a9fe2c7bb125b3409bbe2102577919e"
Jun 20 18:03:17 minikube kubelet[1713]: I0620 18:03:17.720615    1713 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-tvmxf\" (UniqueName: \"kubernetes.io/projected/d57162f0-b718-463f-a69b-5809b4389d3a-kube-api-access-tvmxf\") pod \"d57162f0-b718-463f-a69b-5809b4389d3a\" (UID: \"d57162f0-b718-463f-a69b-5809b4389d3a\") "
Jun 20 18:03:17 minikube kubelet[1713]: I0620 18:03:17.723604    1713 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/d57162f0-b718-463f-a69b-5809b4389d3a-kube-api-access-tvmxf" (OuterVolumeSpecName: "kube-api-access-tvmxf") pod "d57162f0-b718-463f-a69b-5809b4389d3a" (UID: "d57162f0-b718-463f-a69b-5809b4389d3a"). InnerVolumeSpecName "kube-api-access-tvmxf". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jun 20 18:03:17 minikube kubelet[1713]: I0620 18:03:17.821517    1713 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-tvmxf\" (UniqueName: \"kubernetes.io/projected/d57162f0-b718-463f-a69b-5809b4389d3a-kube-api-access-tvmxf\") on node \"minikube\" DevicePath \"\""
Jun 20 18:03:19 minikube kubelet[1713]: I0620 18:03:19.956436    1713 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="d57162f0-b718-463f-a69b-5809b4389d3a" path="/var/lib/kubelet/pods/d57162f0-b718-463f-a69b-5809b4389d3a/volumes"
Jun 20 18:03:25 minikube kubelet[1713]: I0620 18:03:25.917897    1713 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="monitoring/monitoring-prometheus-node-exporter-n7j42" podStartSLOduration=4.892636326 podCreationTimestamp="2025-06-20 18:03:02 +0000 UTC" firstStartedPulling="2025-06-20 18:03:05.814338306 +0000 UTC m=+9214.418943161" lastFinishedPulling="2025-06-20 18:03:24.839399295 +0000 UTC m=+9233.444036898" observedRunningTime="2025-06-20 18:03:25.917293658 +0000 UTC m=+9234.521931261" watchObservedRunningTime="2025-06-20 18:03:25.917730063 +0000 UTC m=+9234.522367666"
Jun 20 18:03:33 minikube kubelet[1713]: I0620 18:03:33.496918    1713 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="monitoring/monitoring-kube-state-metrics-59bd6f86f9-g6t4l" podStartSLOduration=6.001782282 podCreationTimestamp="2025-06-20 18:03:02 +0000 UTC" firstStartedPulling="2025-06-20 18:03:07.115989888 +0000 UTC m=+9215.720594743" lastFinishedPulling="2025-06-20 18:03:32.611043165 +0000 UTC m=+9241.215680768" observedRunningTime="2025-06-20 18:03:33.496405792 +0000 UTC m=+9242.101043495" watchObservedRunningTime="2025-06-20 18:03:33.496868307 +0000 UTC m=+9242.101505910"
Jun 20 18:03:39 minikube kubelet[1713]: I0620 18:03:39.215222    1713 scope.go:117] "RemoveContainer" containerID="588545f0616e3647b2203c967b6de4964a760366a3bc26d8ac2c541d0aad6388"
Jun 20 18:03:39 minikube kubelet[1713]: I0620 18:03:39.422316    1713 scope.go:117] "RemoveContainer" containerID="2b01572ac02e3f4f650636cb1daac7638f0f161d3b15b484f61a9a9f0724a9dd"
Jun 20 18:03:42 minikube kubelet[1713]: I0620 18:03:42.926986    1713 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="monitoring/monitoring-kube-prometheus-operator-559cccc57f-jn9rv" podStartSLOduration=5.640613557 podCreationTimestamp="2025-06-20 18:03:02 +0000 UTC" firstStartedPulling="2025-06-20 18:03:07.318234921 +0000 UTC m=+9215.922839676" lastFinishedPulling="2025-06-20 18:03:42.604474025 +0000 UTC m=+9251.209111728" observedRunningTime="2025-06-20 18:03:42.926386609 +0000 UTC m=+9251.531024312" watchObservedRunningTime="2025-06-20 18:03:42.926885609 +0000 UTC m=+9251.531523312"
Jun 20 18:03:44 minikube kubelet[1713]: I0620 18:03:44.323850    1713 topology_manager.go:215] "Topology Admit Handler" podUID="b609e790-fcf4-4932-bff9-2ee80d05017c" podNamespace="monitoring" podName="alertmanager-monitoring-kube-prometheus-alertmanager-0"
Jun 20 18:03:44 minikube kubelet[1713]: E0620 18:03:44.324069    1713 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="d57162f0-b718-463f-a69b-5809b4389d3a" containerName="patch"
Jun 20 18:03:44 minikube kubelet[1713]: I0620 18:03:44.324239    1713 memory_manager.go:346] "RemoveStaleState removing state" podUID="d57162f0-b718-463f-a69b-5809b4389d3a" containerName="patch"
Jun 20 18:03:44 minikube kubelet[1713]: I0620 18:03:44.427513    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"web-config\" (UniqueName: \"kubernetes.io/secret/b609e790-fcf4-4932-bff9-2ee80d05017c-web-config\") pod \"alertmanager-monitoring-kube-prometheus-alertmanager-0\" (UID: \"b609e790-fcf4-4932-bff9-2ee80d05017c\") " pod="monitoring/alertmanager-monitoring-kube-prometheus-alertmanager-0"
Jun 20 18:03:44 minikube kubelet[1713]: I0620 18:03:44.427718    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cluster-tls-config\" (UniqueName: \"kubernetes.io/secret/b609e790-fcf4-4932-bff9-2ee80d05017c-cluster-tls-config\") pod \"alertmanager-monitoring-kube-prometheus-alertmanager-0\" (UID: \"b609e790-fcf4-4932-bff9-2ee80d05017c\") " pod="monitoring/alertmanager-monitoring-kube-prometheus-alertmanager-0"
Jun 20 18:03:44 minikube kubelet[1713]: I0620 18:03:44.427840    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/secret/b609e790-fcf4-4932-bff9-2ee80d05017c-config-volume\") pod \"alertmanager-monitoring-kube-prometheus-alertmanager-0\" (UID: \"b609e790-fcf4-4932-bff9-2ee80d05017c\") " pod="monitoring/alertmanager-monitoring-kube-prometheus-alertmanager-0"
Jun 20 18:03:44 minikube kubelet[1713]: I0620 18:03:44.427934    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-out\" (UniqueName: \"kubernetes.io/empty-dir/b609e790-fcf4-4932-bff9-2ee80d05017c-config-out\") pod \"alertmanager-monitoring-kube-prometheus-alertmanager-0\" (UID: \"b609e790-fcf4-4932-bff9-2ee80d05017c\") " pod="monitoring/alertmanager-monitoring-kube-prometheus-alertmanager-0"
Jun 20 18:03:44 minikube kubelet[1713]: I0620 18:03:44.428164    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tls-assets\" (UniqueName: \"kubernetes.io/projected/b609e790-fcf4-4932-bff9-2ee80d05017c-tls-assets\") pod \"alertmanager-monitoring-kube-prometheus-alertmanager-0\" (UID: \"b609e790-fcf4-4932-bff9-2ee80d05017c\") " pod="monitoring/alertmanager-monitoring-kube-prometheus-alertmanager-0"
Jun 20 18:03:44 minikube kubelet[1713]: I0620 18:03:44.428313    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"alertmanager-monitoring-kube-prometheus-alertmanager-db\" (UniqueName: \"kubernetes.io/empty-dir/b609e790-fcf4-4932-bff9-2ee80d05017c-alertmanager-monitoring-kube-prometheus-alertmanager-db\") pod \"alertmanager-monitoring-kube-prometheus-alertmanager-0\" (UID: \"b609e790-fcf4-4932-bff9-2ee80d05017c\") " pod="monitoring/alertmanager-monitoring-kube-prometheus-alertmanager-0"
Jun 20 18:03:44 minikube kubelet[1713]: I0620 18:03:44.428444    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mgjvw\" (UniqueName: \"kubernetes.io/projected/b609e790-fcf4-4932-bff9-2ee80d05017c-kube-api-access-mgjvw\") pod \"alertmanager-monitoring-kube-prometheus-alertmanager-0\" (UID: \"b609e790-fcf4-4932-bff9-2ee80d05017c\") " pod="monitoring/alertmanager-monitoring-kube-prometheus-alertmanager-0"
Jun 20 18:03:46 minikube kubelet[1713]: I0620 18:03:46.122806    1713 topology_manager.go:215] "Topology Admit Handler" podUID="d3426bca-1790-4526-866f-a14dc32596a6" podNamespace="monitoring" podName="prometheus-monitoring-kube-prometheus-prometheus-0"
Jun 20 18:03:46 minikube kubelet[1713]: I0620 18:03:46.215952    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0\" (UniqueName: \"kubernetes.io/configmap/d3426bca-1790-4526-866f-a14dc32596a6-prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0\") pod \"prometheus-monitoring-kube-prometheus-prometheus-0\" (UID: \"d3426bca-1790-4526-866f-a14dc32596a6\") " pod="monitoring/prometheus-monitoring-kube-prometheus-prometheus-0"
Jun 20 18:03:46 minikube kubelet[1713]: I0620 18:03:46.216041    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tls-assets\" (UniqueName: \"kubernetes.io/projected/d3426bca-1790-4526-866f-a14dc32596a6-tls-assets\") pod \"prometheus-monitoring-kube-prometheus-prometheus-0\" (UID: \"d3426bca-1790-4526-866f-a14dc32596a6\") " pod="monitoring/prometheus-monitoring-kube-prometheus-prometheus-0"
Jun 20 18:03:46 minikube kubelet[1713]: I0620 18:03:46.216094    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-d57l2\" (UniqueName: \"kubernetes.io/projected/d3426bca-1790-4526-866f-a14dc32596a6-kube-api-access-d57l2\") pod \"prometheus-monitoring-kube-prometheus-prometheus-0\" (UID: \"d3426bca-1790-4526-866f-a14dc32596a6\") " pod="monitoring/prometheus-monitoring-kube-prometheus-prometheus-0"
Jun 20 18:03:46 minikube kubelet[1713]: I0620 18:03:46.216142    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config\" (UniqueName: \"kubernetes.io/secret/d3426bca-1790-4526-866f-a14dc32596a6-config\") pod \"prometheus-monitoring-kube-prometheus-prometheus-0\" (UID: \"d3426bca-1790-4526-866f-a14dc32596a6\") " pod="monitoring/prometheus-monitoring-kube-prometheus-prometheus-0"
Jun 20 18:03:46 minikube kubelet[1713]: I0620 18:03:46.216183    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-out\" (UniqueName: \"kubernetes.io/empty-dir/d3426bca-1790-4526-866f-a14dc32596a6-config-out\") pod \"prometheus-monitoring-kube-prometheus-prometheus-0\" (UID: \"d3426bca-1790-4526-866f-a14dc32596a6\") " pod="monitoring/prometheus-monitoring-kube-prometheus-prometheus-0"
Jun 20 18:03:46 minikube kubelet[1713]: I0620 18:03:46.216244    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"web-config\" (UniqueName: \"kubernetes.io/secret/d3426bca-1790-4526-866f-a14dc32596a6-web-config\") pod \"prometheus-monitoring-kube-prometheus-prometheus-0\" (UID: \"d3426bca-1790-4526-866f-a14dc32596a6\") " pod="monitoring/prometheus-monitoring-kube-prometheus-prometheus-0"
Jun 20 18:03:46 minikube kubelet[1713]: I0620 18:03:46.216354    1713 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"prometheus-monitoring-kube-prometheus-prometheus-db\" (UniqueName: \"kubernetes.io/empty-dir/d3426bca-1790-4526-866f-a14dc32596a6-prometheus-monitoring-kube-prometheus-prometheus-db\") pod \"prometheus-monitoring-kube-prometheus-prometheus-0\" (UID: \"d3426bca-1790-4526-866f-a14dc32596a6\") " pod="monitoring/prometheus-monitoring-kube-prometheus-prometheus-0"
Jun 20 18:03:48 minikube kubelet[1713]: I0620 18:03:48.315822    1713 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="0aca40a0e69c498cd3c813676daffc43d466b9f8d8a546887a2059f6d5d2a897"
Jun 20 18:04:32 minikube kubelet[1713]: W0620 18:04:32.017212    1713 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jun 20 18:04:32 minikube kubelet[1713]: I0620 18:04:32.234801    1713 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="monitoring/alertmanager-monitoring-kube-prometheus-alertmanager-0" podStartSLOduration=6.108611846 podCreationTimestamp="2025-06-20 18:03:44 +0000 UTC" firstStartedPulling="2025-06-20 18:03:48.430659748 +0000 UTC m=+9257.035297351" lastFinishedPulling="2025-06-20 18:04:30.556653431 +0000 UTC m=+9299.161424472" observedRunningTime="2025-06-20 18:04:32.233024862 +0000 UTC m=+9300.837795803" watchObservedRunningTime="2025-06-20 18:04:32.234738967 +0000 UTC m=+9300.839509908"

* 
* ==> kubernetes-dashboard [7269a73e2291] <==
* 2025/06/20 13:12:20 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": net/http: TLS handshake timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00069fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc00004a100)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf
2025/06/20 13:12:20 Using namespace: kubernetes-dashboard
2025/06/20 13:12:20 Using in-cluster config to connect to apiserver
2025/06/20 13:12:20 Using secret token for csrf signing
2025/06/20 13:12:20 Initializing csrf token from kubernetes-dashboard-csrf secret

* 
* ==> kubernetes-dashboard [8eaf17ffcb85] <==
* 2025/06/20 13:12:43 Starting overwatch
2025/06/20 13:12:43 Using namespace: kubernetes-dashboard
2025/06/20 13:12:43 Using in-cluster config to connect to apiserver
2025/06/20 13:12:43 Using secret token for csrf signing
2025/06/20 13:12:43 Initializing csrf token from kubernetes-dashboard-csrf secret
2025/06/20 13:12:43 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2025/06/20 13:12:43 Successful initial request to the apiserver, version: v1.28.3
2025/06/20 13:12:43 Generating JWE encryption key
2025/06/20 13:12:43 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2025/06/20 13:12:43 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2025/06/20 13:12:43 Initializing JWE encryption key from synchronized object
2025/06/20 13:12:43 Creating in-cluster Sidecar client
2025/06/20 13:12:43 Serving insecurely on HTTP port: 9090
2025/06/20 13:12:43 Successful request to sidecar

* 
* ==> storage-provisioner [a910d393a0c1] <==
* I0620 13:12:43.545214       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0620 13:12:43.606384       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0620 13:12:43.608025       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0620 13:13:01.033221       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0620 13:13:01.033372       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"4043298a-9d7a-4aae-9701-f80c3d45bc79", APIVersion:"v1", ResourceVersion:"59862", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_d9d4174c-a706-4fe5-a498-f008cc90b363 became leader
I0620 13:13:01.033455       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_d9d4174c-a706-4fe5-a498-f008cc90b363!
I0620 13:13:01.134824       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_d9d4174c-a706-4fe5-a498-f008cc90b363!

* 
* ==> storage-provisioner [e7d05ebbe299] <==
* I0620 13:12:19.700444       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0620 13:12:29.800719       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout

